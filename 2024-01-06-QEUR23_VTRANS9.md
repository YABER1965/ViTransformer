---
title: QEUR23_VTRANS9: [PyTorch]花の画像を(再度)学習する
date: 2024-01-06
tags: ["QEUシステム", "メトリックス", "Python言語", "Vision Transformer", "LLM", "データセット", "Fine-tuning", "イノベーション"]
excerpt: Vision Transformer(ViT)をやってみる
---

## QEUR23_VTRANS9: [PyTorch]花の画像を(再度)学習する

### ～ 前回のつづき（拡張）になります ～

QEU:FOUNDER ： “それでは、前回のシステムを極力つかって、昔やってた**「花の分類」**をしてみましょう。”

![imageJRL8-10-1](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-1.jpg)

D先生 ： “あのときは、Kerasであり、かつファインチューニングを使いました。今回はPyTorch（HF-Transformer）かつ、新規（直接）学習ですね。しかし、データがすくないでしょう？昔のように、「華麗な学習」ができるかどうか・・・。“

![imageJRL8-10-2](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-2.jpg)

QEU:FOUNDER ： “そんなモン、もともと(高いパフォーマンスが)期待できるわけない。それでは、プロブラムと学習した結果をみてみましょう。例によって、プログラムは、かなり省略しています。ドン・・・。”

```python
##############################
# TRAIN MODEL FROM IMAGES
##############################
#@title ViT Implementation 🔥
import math
import torch
from torch import nn
from torchinfo import summary

class NewGELUActivation(nn.Module):
    """
    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see
    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415
    Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py
    """
    def forward(self, input):
        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))

# ------
class PatchEmbeddings(nn.Module):
    """
    Convert the image into patches and then project them into a vector space.
    """
省略
        return x

# ------
class Embeddings(nn.Module):
    """
    Combine the patch embeddings with the class token and position embeddings.
    """
省略
        return x

# ------
class AttentionHead(nn.Module):
    """
    A single attention head.
    This module is used in the MultiHeadAttention module.
    """
省略
        return (attention_output, attention_probs)

# ------
class MultiHeadAttention(nn.Module):
    """
    Multi-head attention module.
    This module is used in the TransformerEncoder module.
    """
省略
        if not output_attentions:
            return (attention_output, None)
        else:
            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)
            return (attention_output, attention_probs)

# ------
class FasterMultiHeadAttention(nn.Module):
    """
    Multi-head attention module with some optimizations.
    All the heads are processed simultaneously with merged query, key, and value projections.
    """
省略
        if not output_attentions:
            return (attention_output, None)
        else:
            return (attention_output, attention_probs)

# ------
class MLP(nn.Module):
    """
    A multi-layer perceptron module.
    """
省略
        return x

# ------
class Block(nn.Module):
    """
    A single transformer block.
    """
省略
        if not output_attentions:
            return (x, None)
        else:
            return (x, attention_probs)

# ------
class Encoder(nn.Module):
    """
    The transformer encoder module.
    """
省略
        if not output_attentions:
            return (x, None)
        else:
            return (x, all_attentions)

# ------
class ViTForClassfication(nn.Module):
    """
    The ViT model for classification.
    """
省略


# ------
#@title Prepare Data 📊
# Import libraries
import torch
import torchvision
import torchvision.transforms as transforms

# ------
# 学習データを準備する
def prepare_data(batch_size):

省略

    # ----
    classes = ('daisy', 'dandelion', 'roses', 'sunflowers', 'tulips')
    
    return trainloader, testloader, classes
     
```

![imageJRL8-10-3](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-3.jpg)

C部長 : “あれ？こんな画像集（↑）がプログラムで出ましたっけ？”

QEU:FOUNDER ： “我々はインプット画像の出力用のプログラムを作りましたが、ここでは紹介していません。簡単ですから、各自、作成してください。ここで言いたかったことは、プログラムのこの部分（↑）でデータセットを作成したことと、データセットが**224x224**の画像データでできているということです。”

C部長 : “CIPHAR10の画像のサイズは32X32でしたよね。これ（画像のサイズアップ）は、画像の予測にとって大きなアドバンテージだ・・・。でも、同時に他のパラメータを変えなければならないですね。”

D先生 ： “ViT(Vision Transformer)では、**画像を大きくしたアドバンテージを発揮するにはパッチを大きくしなければなりません**。今回は16x16にしています。”


```python
# ------
#@title Utils 🛠️
import json, os, math
import matplotlib.pyplot as plt
import numpy as np
import torch
from torch.nn import functional as F
import torchvision
import torchvision.transforms as transforms

# ---
def save_experiment(experiment_name, config, model, train_losses, test_losses, accuracies, base_dir="experiments"):
    outdir = os.path.join(base_dir, experiment_name)
    os.makedirs(outdir, exist_ok=True)

    # Save the config
    configfile = os.path.join(outdir, 'config.json')
    with open(configfile, 'w') as f:
        json.dump(config, f, sort_keys=True, indent=4)

    # Save the metrics
    jsonfile = os.path.join(outdir, 'metrics.json')
    with open(jsonfile, 'w') as f:
        data = {
            'train_losses': train_losses,
            'test_losses': test_losses,
            'accuracies': accuracies,
        }
        json.dump(data, f, sort_keys=True, indent=4)

    # Save the model
    save_checkpoint(experiment_name, model, "final", base_dir=base_dir)

def save_checkpoint(experiment_name, model, epoch, base_dir="experiments"):
    outdir = os.path.join(base_dir, experiment_name)
    os.makedirs(outdir, exist_ok=True)
    cpfile = os.path.join(outdir, f'model_{epoch}.pt')
    torch.save(model.state_dict(), cpfile)

# -----
#@title Train ViT 🧠 🏋🏽
#@title String fields
exp_name = 'vit-flower-20-epochs' #@param {type:"string"}
batch_size = 32 #@param {type: "integer"}
epochs = 20 #@param {type: "integer"}
lr = 1e-3  #@param {type: "number"}
save_model_every = 0 #@param {type: "integer"}
# ---
import torch
from torch import nn, optim
# ---
device = "cuda" if torch.cuda.is_available() else "cpu"
# ---
config = {
    "patch_size": 16,  # Input image size: 32x32 -> 8x8 patches
    "hidden_size": 48,
    "num_hidden_layers": 4,
    "num_attention_heads": 4,
    "intermediate_size": 4 * 48, # 4 * hidden_size
    "hidden_dropout_prob": 0.0,
    "attention_probs_dropout_prob": 0.0,
    "initializer_range": 0.02,
    "image_size": 224,
    "num_classes": 5, # num_classes of flower data
    "num_channels": 3,
    "qkv_bias": True,
    "use_faster_attention": True,
}
# These are not hard constraints, but are used to prevent misconfigurations
assert config["hidden_size"] % config["num_attention_heads"] == 0
assert config['intermediate_size'] == 4 * config['hidden_size']
assert config['image_size'] % config['patch_size'] == 0

# -------------
# START TRAINING
# -------------
class Trainer:
    """
    The simple trainer.
    """
    def __init__(self, model, optimizer, loss_fn, exp_name, device):
        self.model = model.to(device)
        self.optimizer = optimizer
        self.loss_fn = loss_fn
        self.exp_name = exp_name
        self.device = device

    def train(self, trainloader, testloader, epochs, save_model_every_n_epochs=0):
        """
        Train the model for the specified number of epochs.
        """
        # Keep track of the losses and accuracies
        train_losses, test_losses, accuracies = [], [], []
        # Train the model
        for i in range(epochs):
            train_loss = self.train_epoch(trainloader)
            accuracy, test_loss = self.evaluate(testloader)
            train_losses.append(train_loss)
            test_losses.append(test_loss)
            accuracies.append(accuracy)
            print(f"Epoch: {i+1}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {accura-cy:.4f}")
            if save_model_every_n_epochs > 0 and (i+1) % save_model_every_n_epochs == 0 and i+1 != epochs:
                print('\tSave checkpoint at epoch', i+1)
                save_checkpoint(self.exp_name, self.model, i+1)
        # Save the experiment
        save_experiment(self.exp_name, config, self.model, train_losses, test_losses, accuracies)

    def train_epoch(self, trainloader):
        """
        Train the model for one epoch.
        """
        self.model.train()
        total_loss = 0
        for batch in trainloader:
            # Move the batch to the device
            batch = [t.to(self.device) for t in batch]
            images, labels = batch
            # Zero the gradients
            self.optimizer.zero_grad()
            # Calculate the loss
            loss = self.loss_fn(self.model(images)[0], labels)
            # Backpropagate the loss
            loss.backward()
            # Update the model's parameters
            self.optimizer.step()
            total_loss += loss.item() * len(images)
        return total_loss / len(trainloader.dataset)

    @torch.no_grad()
    def evaluate(self, testloader):
        self.model.eval()
        total_loss = 0
        correct = 0
        with torch.no_grad():
            for batch in testloader:
                # Move the batch to the device
                batch = [t.to(self.device) for t in batch]
                images, labels = batch

                # Get predictions
                logits, _ = self.model(images)

                # Calculate the loss
                loss = self.loss_fn(logits, labels)
                total_loss += loss.item() * len(images)

                # Calculate the accuracy
                predictions = torch.argmax(logits, dim=1)
                correct += torch.sum(predictions == labels).item()
        accuracy = correct / len(testloader.dataset)
        avg_loss = total_loss / len(testloader.dataset)
        return accuracy, avg_loss

# -------
# Training parameters
save_model_every_n_epochs = save_model_every
# Create the flower dataset
trainloader, testloader, classes = prepare_data(BATCH_SIZE)
# Create the model, optimizer, loss function and trainer
model = ViTForClassfication(config)

# -------
# display model's structure
summary(model=model,
        input_size=(BATCH_SIZE, 3, 224, 224), # (batch_size, num_patches, embedding_dimension)
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"])

```

C部長 : “ああっ！この**ViTモデルのまとめ方がイケてる**！これ（↓）、前回の表現方法と変わりましたが、なんですか？”

![imageJRL8-10-4](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-4.jpg)

QEU:FOUNDER ： “**torchinfoというライブラリのsummaryメソッド**を使ってみました。”

![imageJRL8-10-5](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-5.jpg)

D先生 ： “やっぱり、このモデルは簡単ですね。Kerasのファインチューニングで使ったモデルは**パラメタ数が86M**でした。”

QEU:FOUNDER ： “そして、最後のパートが「学習(↓)」です。”

```python
# -----
# SUBMIT TRAINING
optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=2e-3)
loss_fn = nn.CrossEntropyLoss()
trainer = Trainer(model, optimizer, loss_fn, exp_name, device=device)
trainer.train(trainloader, testloader, epochs, save_model_every_n_epochs=save_model_every_n_epochs)
```

![imageJRL8-10-6](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-6.jpg)

C部長 : “**Test_Lossの増加**ぶりにワロタ・・・。”

![imageJRL8-10-7](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-7.jpg)

QEU:FOUNDER ： “これは、ある意味しようがないんですよ。以前と同じくTEST画像では認識の難易度が高い画像を使っているんですから・・・。それでは、おまちかねの**「アテンション・マップ(Attention map)」**をつづけて作成します。”

```python
##############################
# ATTENTON MAP(INCLUDING LOAD MODEL)
##############################
#@title ViT Implementation 🔥
import math
import torch
from torch import nn

以下、一気に省略

# ------
# Create Dataset and DataLoader
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader

# ------
test_transform = transforms.Compose(
    [transforms.ToTensor(),
#    transforms.Resize((32, 32)),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

test_dataset = ImageFolder(root="flower_photos_GTEST", transform=test_transform)
testloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)

# -------
# 結果を予測する(バッチ分だけ出力する)
model.eval()
with torch.no_grad():
    icount = 0
    for batch in testloader:
        if icount == 0:
            # Move the batch to the device
            batch = [t.to(device) for t in batch]
            images, labels = batch
            #print(images.shape)
            # Get predictions
            model = model.to(device)
            logits, attention_maps = model(images, output_attentions=True)
            #print(len(attention_maps))
            #print(attention_maps[0].shape)
            #print(logits)
            #print(labels)
            icount = icount + 1
# ----
# イメージとラベルを生成する
np_images = images.cpu().detach().numpy()
np_images = np.transpose(np_images, (0,2,3,1))
np_labels = labels.cpu().detach().numpy()
# ----
# 予測とアテンション・マップを生成する
predictions = torch.argmax(logits, dim=1)
np_predictions = predictions.cpu().detach().numpy()
# Concatenate the attention maps from all blocks
attention_maps = torch.cat(attention_maps, dim=1)
# select only the attention maps of the CLS token
attention_maps = attention_maps[:, :, 0, 1:]
np_attention_maps = attention_maps.cpu().detach().numpy()

# ------
# アテンションの可視化
def visualize_attention(num_images, images, labels, predictions, attention_maps, output):
    """
    Visualize the attention maps of the first 9 images.
    """
    classes = ('daisy', 'dandelion', 'roses', 'sunflowers', 'tulips')
    # Pick images
    raw_images = [np.asarray(images[i]) for i in range(num_images)]
    labels = [labels[i] for i in range(num_images)]
    # ---
    # Then average the attention maps of the CLS token over all the heads
    attention_maps = attention_maps.mean(dim=1)
    # Reshape the attention maps to a square
    num_patches = attention_maps.size(-1)
    size = int(math.sqrt(num_patches))
    attention_maps = attention_maps.view(-1, size, size)
    # Resize the map to the size of the image
    attention_maps = attention_maps.unsqueeze(1)
    attention_maps = F.interpolate(attention_maps, size=(224, 224), mode='bilinear', align_corners=False)
    attention_maps = attention_maps.squeeze(1)
    # Plot the images and the attention maps
    fig = plt.figure(figsize=(16, 12))
    mask = np.concatenate([np.ones((224, 224)), np.zeros((224, 224))], axis=1)
    for i in range(num_images):
        ax = fig.add_subplot(3, 3, i+1, xticks=[], yticks=[])
        img = np.concatenate((raw_images[i], raw_images[i]), axis=1)
        ax.imshow(img)
        # Mask out the attention map of the left image
        extended_attention_map = np.concatenate((np.zeros((224, 224)), attention_maps[i].cpu()), axis=1)
        extended_attention_map = np.ma.masked_where(mask==1, extended_attention_map)
        ax.imshow(extended_attention_map, alpha=0.5, cmap='jet')
        # Show the ground truth and the prediction
        gt = classes[labels[i]]
        pred = classes[predictions[i]]
        ax.set_title(f"gt: {gt} / pred: {pred}", color=("green" if gt==pred else "red"))
    plt.tight_layout()
    if output is not None:
        plt.savefig(output)
    plt.show()

# ------
num_images = 9

# ------
#@title Visualize Attetion
visualize_attention(num_images, np_images, np_labels, np_predictions, attention_maps, "atten-tion_flower_sono1.png")

```

QEU:FOUNDER ： “このアテンション・マップ作製用のプログラムは、初めにテストデータセットと学習済のモデルを読み込んでいます。ですから、学習用プログラムを実行し、それから続けてマップを生成したい場合には前半のプログラムは不要です。**だから前半のプログラムの説明は省略しました**。そして、肝心のアテンション・マップの作製結果をドン！”

![imageJRL8-10-8](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-8.jpg)

D先生 ： “やっぱり、224x224の画像はきれいだ。でも、元画像の色が少し変わっていません？”

QEU:FOUNDER  ： “これはデータセットを作成するときに(0,225)の画像データを(0,1)データに変換したので起きたものです。元画像は特に重要でもないので、このままにしています。”

**（Keras-Fine-tuning:その１）**

![imageJRL8-10-9](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-9.jpg)

C部長 : “う～ん・・・、断定はできないです。・・・でも、Keras-ファインチューニングを使ったときよりも、**アテンションの分布が良くなっている**気がします。どうですか？D先生・・・。”

**（Keras-Fine-tuning:その２）**

![imageJRL8-10-10](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-10.jpg)

D先生 ： “同意します。今回は、**アテンションが「花」に向いています**。そういえば・・・、ひとつ質問があります。今回のプログラムで、アテンション・マップを作る関数の構造がかなり変わったのはなぜですか？本来は、そんなことをする必要がないはずなのに・・・。”

QEU:FOUNDER  ： “小生のプログラミング能力の限界です。前回のプログラムのように個別の画像をデータセット化する方法ではどうしてもアテンションマップ生成の関数上でエラーが出て動かなかったので、学習と同様にデータセットを計算して入力するようにしたんです。もちろん、これは良い方法ではないが、皆さんで改造・修正をお願いします。”

D先生 ： “自らの無能を吐露した、とても正直でよろしい・・・（笑）。”

QEU:FOUNDER（設定年齢65歳） ： “この程度のデジタル技術を知っている若者は世の中にたくさんいます。ただし、彼らはこのテクノロジをどのように使うべきか（アプリケーション）を知らないんです。それを**「橋渡しする仕事」**を小生が行っているわけです。”

### [＞寄付のお願い(click here)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

QEU:FOUNDER  ： “**老骨に鞭打って・・・。**”

## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

D先生 ： “風の時代が始まり、C国の巻き返しが始まりました。EV車だけじゃなく、なんとC国製のエンジン車も売れているらしい。一体、海外では何が起きているんでしょうね？”

![imageJRL8-10-11](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-11.jpg)

QEU:FOUNDER ： “まずは、モノの品質に差がなくなっているんでしょう。それは当たり前だよね。もっと重要なのは、「Made in PRC」のイメージが変わったこと・・・。**EV車の販売を通じて、（C国企業は）一気に先進的だと思われるようになった**よね。”

D先生 ： “ぐぬぬ・・・。では、J国（企業）の巻き返しがあるんでしょうかね？”

QEU:FOUNDER ： “**サプライチェーンの考え方を変えれば、イメージの巻き返しは可能かもしれない**。サプライヤーは、部品を供給してもらう会社ではなく、自社製品を買ってもらう「お客様がいらっしゃる」会社だと思うといいと思います。サプライアの作業員がミスをしたとしても、顧客が自らが設計した検査機で見つければいい。これこそ**「品質のレジリエンス」**！！その作業員は救われた感激のあまり、一気にファンになって、そこの従業員全員が大量に製品を買ってくれるでしょう。**風の時代(2024～)において、「サプライチェーンは搾取するシステムではない」**んですよ。”

D先生 ： “FOUNDERの言っていることが突飛すぎて、普通の人は訳が分からないともいますよ。ちなみに、私も最初は「キョトン・・・」としました・・・（笑）。”

![imageJRL8-10-12](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-12.jpg)

QEU:FOUNDER ： “この理論のポイントは**「近年の急激な技術革新」**です。（いわゆる）新興国では、「軽工業→重工業→電子工業etc」なんかという、産業革命からの人類の科学技術発展のゆっくりしたステップをふまずに、デジタル化されたテクノロジを使って、いきなり産業が成長するんです。だから、そこに住む人たちは昔のことは気にしない。オッサンの、**「昔、俺はすごかったんだぞ！」**という昔話は、もはやだれも聞いていない・・・。”

### オッサン： 従業員の皆さんにはテレビを見てください。皆が同じように考えてください。
### オッサン：“検査不正”っていうのはなァ、（組織外に不正を）漏らしたヤツが悪いんだよ。
### オッサン：私の使命はこの会社で終身雇用制を実現することにある。**(わざわざ201X年で、かつ海外の会社でこれを言うか！？**おめでとう！あの国、これからバンバン伸びるよ！**)

C部長 : “なるほど。鼻をほじって、「ふ～ん」という感じ・・・（笑）。”

![imageJRL8-10-13](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-13.jpg)

QEU:FOUNDER ： “だから、彼らは古い技術や製品に対しても関心が薄いわけ・・・。さらにいうと、彼ら消費者は、できれば自分の国で作ったものを消費したいわけ、それは当たり前の心情だよね。さて、その一方で・・・。”

![imageJRL8-10-14](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-14.jpg)

QEU:FOUNDER ： “今までの話を逆にいうと、（新興国の）若者の場合、**モノやメーカーの良し悪しの評価スケールも新しい技術がベースになる**わけです。ちなみに、前回のプログラムはV国の若者が作ったGithubの内容がベースになりました。そういった彼らが、**「未来の消費者」**となるわけです・・・。”

D先生 ： “それはロートル共には厳しい・・・。そういう人たちにアピールするには？”

[![MOVIE1](http://img.youtube.com/vi/e8aiitAAAZY/0.jpg)](http://www.youtube.com/watch?v=e8aiitAAAZY "怒！厚顔無恥＆無知の豊田会長の発言。「なぜ日本人の給料が上がらないのか」について自工会の豊田章男会長が仰天発言。経済界のボスは竹中平蔵と同レベルか。")

QEU:FOUNDER ： “こんなアピール（↑）はいかが？”

D先生 ： “シバかれますよ。若者に・・・。”
