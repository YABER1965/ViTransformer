---
title: QEUR23_VTRANS9: [PyTorch]èŠ±ã®ç”»åƒã‚’(å†åº¦)å­¦ç¿’ã™ã‚‹
date: 2024-01-06
tags: ["QEUã‚·ã‚¹ãƒ†ãƒ ", "ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹", "Pythonè¨€èª", "Vision Transformer", "LLM", "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ", "Fine-tuning", "ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³"]
excerpt: Vision Transformer(ViT)ã‚’ã‚„ã£ã¦ã¿ã‚‹
---

## QEUR23_VTRANS9: [PyTorch]èŠ±ã®ç”»åƒã‚’(å†åº¦)å­¦ç¿’ã™ã‚‹

### ï½ å‰å›ã®ã¤ã¥ãï¼ˆæ‹¡å¼µï¼‰ã«ãªã‚Šã¾ã™ ï½

QEU:FOUNDER ï¼š â€œãã‚Œã§ã¯ã€å‰å›ã®ã‚·ã‚¹ãƒ†ãƒ ã‚’æ¥µåŠ›ã¤ã‹ã£ã¦ã€æ˜”ã‚„ã£ã¦ãŸ**ã€ŒèŠ±ã®åˆ†é¡ã€**ã‚’ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚â€

![imageJRL8-10-1](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-1.jpg)

Då…ˆç”Ÿ ï¼š â€œã‚ã®ã¨ãã¯ã€Kerasã§ã‚ã‚Šã€ã‹ã¤ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ä½¿ã„ã¾ã—ãŸã€‚ä»Šå›ã¯PyTorchï¼ˆHF-Transformerï¼‰ã‹ã¤ã€æ–°è¦ï¼ˆç›´æ¥ï¼‰å­¦ç¿’ã§ã™ã­ã€‚ã—ã‹ã—ã€ãƒ‡ãƒ¼ã‚¿ãŒã™ããªã„ã§ã—ã‚‡ã†ï¼Ÿæ˜”ã®ã‚ˆã†ã«ã€ã€Œè¯éº—ãªå­¦ç¿’ã€ãŒã§ãã‚‹ã‹ã©ã†ã‹ãƒ»ãƒ»ãƒ»ã€‚â€œ

![imageJRL8-10-2](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-2.jpg)

QEU:FOUNDER ï¼š â€œãã‚“ãªãƒ¢ãƒ³ã€ã‚‚ã¨ã‚‚ã¨(é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒ)æœŸå¾…ã§ãã‚‹ã‚ã‘ãªã„ã€‚ãã‚Œã§ã¯ã€ãƒ—ãƒ­ãƒ–ãƒ©ãƒ ã¨å­¦ç¿’ã—ãŸçµæœã‚’ã¿ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ä¾‹ã«ã‚ˆã£ã¦ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯ã€ã‹ãªã‚Šçœç•¥ã—ã¦ã„ã¾ã™ã€‚ãƒ‰ãƒ³ãƒ»ãƒ»ãƒ»ã€‚â€

```python
##############################
# TRAIN MODEL FROM IMAGES
##############################
#@title ViT Implementation ğŸ”¥
import math
import torch
from torch import nn
from torchinfo import summary

class NewGELUActivation(nn.Module):
    """
    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see
    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415
    Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py
    """
    def forward(self, input):
        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))

# ------
class PatchEmbeddings(nn.Module):
    """
    Convert the image into patches and then project them into a vector space.
    """
çœç•¥
        return x

# ------
class Embeddings(nn.Module):
    """
    Combine the patch embeddings with the class token and position embeddings.
    """
çœç•¥
        return x

# ------
class AttentionHead(nn.Module):
    """
    A single attention head.
    This module is used in the MultiHeadAttention module.
    """
çœç•¥
        return (attention_output, attention_probs)

# ------
class MultiHeadAttention(nn.Module):
    """
    Multi-head attention module.
    This module is used in the TransformerEncoder module.
    """
çœç•¥
        if not output_attentions:
            return (attention_output, None)
        else:
            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)
            return (attention_output, attention_probs)

# ------
class FasterMultiHeadAttention(nn.Module):
    """
    Multi-head attention module with some optimizations.
    All the heads are processed simultaneously with merged query, key, and value projections.
    """
çœç•¥
        if not output_attentions:
            return (attention_output, None)
        else:
            return (attention_output, attention_probs)

# ------
class MLP(nn.Module):
    """
    A multi-layer perceptron module.
    """
çœç•¥
        return x

# ------
class Block(nn.Module):
    """
    A single transformer block.
    """
çœç•¥
        if not output_attentions:
            return (x, None)
        else:
            return (x, attention_probs)

# ------
class Encoder(nn.Module):
    """
    The transformer encoder module.
    """
çœç•¥
        if not output_attentions:
            return (x, None)
        else:
            return (x, all_attentions)

# ------
class ViTForClassfication(nn.Module):
    """
    The ViT model for classification.
    """
çœç•¥


# ------
#@title Prepare Data ğŸ“Š
# Import libraries
import torch
import torchvision
import torchvision.transforms as transforms

# ------
# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹
def prepare_data(batch_size):

çœç•¥

    # ----
    classes = ('daisy', 'dandelion', 'roses', 'sunflowers', 'tulips')
    
    return trainloader, testloader, classes
     
```

![imageJRL8-10-3](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-3.jpg)

Céƒ¨é•· : â€œã‚ã‚Œï¼Ÿã“ã‚“ãªç”»åƒé›†ï¼ˆâ†‘ï¼‰ãŒãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§å‡ºã¾ã—ãŸã£ã‘ï¼Ÿâ€

QEU:FOUNDER ï¼š â€œæˆ‘ã€…ã¯ã‚¤ãƒ³ãƒ—ãƒƒãƒˆç”»åƒã®å‡ºåŠ›ç”¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ä½œã‚Šã¾ã—ãŸãŒã€ã“ã“ã§ã¯ç´¹ä»‹ã—ã¦ã„ã¾ã›ã‚“ã€‚ç°¡å˜ã§ã™ã‹ã‚‰ã€å„è‡ªã€ä½œæˆã—ã¦ãã ã•ã„ã€‚ã“ã“ã§è¨€ã„ãŸã‹ã£ãŸã“ã¨ã¯ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ã“ã®éƒ¨åˆ†ï¼ˆâ†‘ï¼‰ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ãŸã“ã¨ã¨ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒ**224x224**ã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã§ã§ãã¦ã„ã‚‹ã¨ã„ã†ã“ã¨ã§ã™ã€‚â€

Céƒ¨é•· : â€œCIPHAR10ã®ç”»åƒã®ã‚µã‚¤ã‚ºã¯32X32ã§ã—ãŸã‚ˆã­ã€‚ã“ã‚Œï¼ˆç”»åƒã®ã‚µã‚¤ã‚ºã‚¢ãƒƒãƒ—ï¼‰ã¯ã€ç”»åƒã®äºˆæ¸¬ã«ã¨ã£ã¦å¤§ããªã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã ãƒ»ãƒ»ãƒ»ã€‚ã§ã‚‚ã€åŒæ™‚ã«ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰ãˆãªã‘ã‚Œã°ãªã‚‰ãªã„ã§ã™ã­ã€‚â€

Då…ˆç”Ÿ ï¼š â€œViT(Vision Transformer)ã§ã¯ã€**ç”»åƒã‚’å¤§ããã—ãŸã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã‚’ç™ºæ®ã™ã‚‹ã«ã¯ãƒ‘ãƒƒãƒã‚’å¤§ããã—ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“**ã€‚ä»Šå›ã¯16x16ã«ã—ã¦ã„ã¾ã™ã€‚â€


```python
# ------
#@title Utils ğŸ› ï¸
import json, os, math
import matplotlib.pyplot as plt
import numpy as np
import torch
from torch.nn import functional as F
import torchvision
import torchvision.transforms as transforms

# ---
def save_experiment(experiment_name, config, model, train_losses, test_losses, accuracies, base_dir="experiments"):
    outdir = os.path.join(base_dir, experiment_name)
    os.makedirs(outdir, exist_ok=True)

    # Save the config
    configfile = os.path.join(outdir, 'config.json')
    with open(configfile, 'w') as f:
        json.dump(config, f, sort_keys=True, indent=4)

    # Save the metrics
    jsonfile = os.path.join(outdir, 'metrics.json')
    with open(jsonfile, 'w') as f:
        data = {
            'train_losses': train_losses,
            'test_losses': test_losses,
            'accuracies': accuracies,
        }
        json.dump(data, f, sort_keys=True, indent=4)

    # Save the model
    save_checkpoint(experiment_name, model, "final", base_dir=base_dir)

def save_checkpoint(experiment_name, model, epoch, base_dir="experiments"):
    outdir = os.path.join(base_dir, experiment_name)
    os.makedirs(outdir, exist_ok=True)
    cpfile = os.path.join(outdir, f'model_{epoch}.pt')
    torch.save(model.state_dict(), cpfile)

# -----
#@title Train ViT ğŸ§  ğŸ‹ğŸ½
#@title String fields
exp_name = 'vit-flower-20-epochs' #@param {type:"string"}
batch_size = 32 #@param {type: "integer"}
epochs = 20 #@param {type: "integer"}
lr = 1e-3  #@param {type: "number"}
save_model_every = 0 #@param {type: "integer"}
# ---
import torch
from torch import nn, optim
# ---
device = "cuda" if torch.cuda.is_available() else "cpu"
# ---
config = {
    "patch_size": 16,  # Input image size: 32x32 -> 8x8 patches
    "hidden_size": 48,
    "num_hidden_layers": 4,
    "num_attention_heads": 4,
    "intermediate_size": 4 * 48, # 4 * hidden_size
    "hidden_dropout_prob": 0.0,
    "attention_probs_dropout_prob": 0.0,
    "initializer_range": 0.02,
    "image_size": 224,
    "num_classes": 5, # num_classes of flower data
    "num_channels": 3,
    "qkv_bias": True,
    "use_faster_attention": True,
}
# These are not hard constraints, but are used to prevent misconfigurations
assert config["hidden_size"] % config["num_attention_heads"] == 0
assert config['intermediate_size'] == 4 * config['hidden_size']
assert config['image_size'] % config['patch_size'] == 0

# -------------
# START TRAINING
# -------------
class Trainer:
    """
    The simple trainer.
    """
    def __init__(self, model, optimizer, loss_fn, exp_name, device):
        self.model = model.to(device)
        self.optimizer = optimizer
        self.loss_fn = loss_fn
        self.exp_name = exp_name
        self.device = device

    def train(self, trainloader, testloader, epochs, save_model_every_n_epochs=0):
        """
        Train the model for the specified number of epochs.
        """
        # Keep track of the losses and accuracies
        train_losses, test_losses, accuracies = [], [], []
        # Train the model
        for i in range(epochs):
            train_loss = self.train_epoch(trainloader)
            accuracy, test_loss = self.evaluate(testloader)
            train_losses.append(train_loss)
            test_losses.append(test_loss)
            accuracies.append(accuracy)
            print(f"Epoch: {i+1}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {accura-cy:.4f}")
            if save_model_every_n_epochs > 0 and (i+1) % save_model_every_n_epochs == 0 and i+1 != epochs:
                print('\tSave checkpoint at epoch', i+1)
                save_checkpoint(self.exp_name, self.model, i+1)
        # Save the experiment
        save_experiment(self.exp_name, config, self.model, train_losses, test_losses, accuracies)

    def train_epoch(self, trainloader):
        """
        Train the model for one epoch.
        """
        self.model.train()
        total_loss = 0
        for batch in trainloader:
            # Move the batch to the device
            batch = [t.to(self.device) for t in batch]
            images, labels = batch
            # Zero the gradients
            self.optimizer.zero_grad()
            # Calculate the loss
            loss = self.loss_fn(self.model(images)[0], labels)
            # Backpropagate the loss
            loss.backward()
            # Update the model's parameters
            self.optimizer.step()
            total_loss += loss.item() * len(images)
        return total_loss / len(trainloader.dataset)

    @torch.no_grad()
    def evaluate(self, testloader):
        self.model.eval()
        total_loss = 0
        correct = 0
        with torch.no_grad():
            for batch in testloader:
                # Move the batch to the device
                batch = [t.to(self.device) for t in batch]
                images, labels = batch

                # Get predictions
                logits, _ = self.model(images)

                # Calculate the loss
                loss = self.loss_fn(logits, labels)
                total_loss += loss.item() * len(images)

                # Calculate the accuracy
                predictions = torch.argmax(logits, dim=1)
                correct += torch.sum(predictions == labels).item()
        accuracy = correct / len(testloader.dataset)
        avg_loss = total_loss / len(testloader.dataset)
        return accuracy, avg_loss

# -------
# Training parameters
save_model_every_n_epochs = save_model_every
# Create the flower dataset
trainloader, testloader, classes = prepare_data(BATCH_SIZE)
# Create the model, optimizer, loss function and trainer
model = ViTForClassfication(config)

# -------
# display model's structure
summary(model=model,
        input_size=(BATCH_SIZE, 3, 224, 224), # (batch_size, num_patches, embedding_dimension)
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"])

```

Céƒ¨é•· : â€œã‚ã‚ã£ï¼ã“ã®**ViTãƒ¢ãƒ‡ãƒ«ã®ã¾ã¨ã‚æ–¹ãŒã‚¤ã‚±ã¦ã‚‹**ï¼ã“ã‚Œï¼ˆâ†“ï¼‰ã€å‰å›ã®è¡¨ç¾æ–¹æ³•ã¨å¤‰ã‚ã‚Šã¾ã—ãŸãŒã€ãªã‚“ã§ã™ã‹ï¼Ÿâ€

![imageJRL8-10-4](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-4.jpg)

QEU:FOUNDER ï¼š â€œ**torchinfoã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®summaryãƒ¡ã‚½ãƒƒãƒ‰**ã‚’ä½¿ã£ã¦ã¿ã¾ã—ãŸã€‚â€

![imageJRL8-10-5](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-5.jpg)

Då…ˆç”Ÿ ï¼š â€œã‚„ã£ã±ã‚Šã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ç°¡å˜ã§ã™ã­ã€‚Kerasã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ä½¿ã£ãŸãƒ¢ãƒ‡ãƒ«ã¯**ãƒ‘ãƒ©ãƒ¡ã‚¿æ•°ãŒ86M**ã§ã—ãŸã€‚â€

QEU:FOUNDER ï¼š â€œãã—ã¦ã€æœ€å¾Œã®ãƒ‘ãƒ¼ãƒˆãŒã€Œå­¦ç¿’(â†“)ã€ã§ã™ã€‚â€

```python
# -----
# SUBMIT TRAINING
optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=2e-3)
loss_fn = nn.CrossEntropyLoss()
trainer = Trainer(model, optimizer, loss_fn, exp_name, device=device)
trainer.train(trainloader, testloader, epochs, save_model_every_n_epochs=save_model_every_n_epochs)
```

![imageJRL8-10-6](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-6.jpg)

Céƒ¨é•· : â€œ**Test_Lossã®å¢—åŠ **ã¶ã‚Šã«ãƒ¯ãƒ­ã‚¿ãƒ»ãƒ»ãƒ»ã€‚â€

![imageJRL8-10-7](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-7.jpg)

QEU:FOUNDER ï¼š â€œã“ã‚Œã¯ã€ã‚ã‚‹æ„å‘³ã—ã‚ˆã†ãŒãªã„ã‚“ã§ã™ã‚ˆã€‚ä»¥å‰ã¨åŒã˜ãTESTç”»åƒã§ã¯èªè­˜ã®é›£æ˜“åº¦ãŒé«˜ã„ç”»åƒã‚’ä½¿ã£ã¦ã„ã‚‹ã‚“ã§ã™ã‹ã‚‰ãƒ»ãƒ»ãƒ»ã€‚ãã‚Œã§ã¯ã€ãŠã¾ã¡ã‹ã­ã®**ã€Œã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒãƒƒãƒ—(Attention map)ã€**ã‚’ã¤ã¥ã‘ã¦ä½œæˆã—ã¾ã™ã€‚â€

```python
##############################
# ATTENTON MAP(INCLUDING LOAD MODEL)
##############################
#@title ViT Implementation ğŸ”¥
import math
import torch
from torch import nn

ä»¥ä¸‹ã€ä¸€æ°—ã«çœç•¥

# ------
# Create Dataset and DataLoader
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader

# ------
test_transform = transforms.Compose(
    [transforms.ToTensor(),
#    transforms.Resize((32, 32)),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

test_dataset = ImageFolder(root="flower_photos_GTEST", transform=test_transform)
testloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)

# -------
# çµæœã‚’äºˆæ¸¬ã™ã‚‹(ãƒãƒƒãƒåˆ†ã ã‘å‡ºåŠ›ã™ã‚‹)
model.eval()
with torch.no_grad():
    icount = 0
    for batch in testloader:
        if icount == 0:
            # Move the batch to the device
            batch = [t.to(device) for t in batch]
            images, labels = batch
            #print(images.shape)
            # Get predictions
            model = model.to(device)
            logits, attention_maps = model(images, output_attentions=True)
            #print(len(attention_maps))
            #print(attention_maps[0].shape)
            #print(logits)
            #print(labels)
            icount = icount + 1
# ----
# ã‚¤ãƒ¡ãƒ¼ã‚¸ã¨ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆã™ã‚‹
np_images = images.cpu().detach().numpy()
np_images = np.transpose(np_images, (0,2,3,1))
np_labels = labels.cpu().detach().numpy()
# ----
# äºˆæ¸¬ã¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒãƒƒãƒ—ã‚’ç”Ÿæˆã™ã‚‹
predictions = torch.argmax(logits, dim=1)
np_predictions = predictions.cpu().detach().numpy()
# Concatenate the attention maps from all blocks
attention_maps = torch.cat(attention_maps, dim=1)
# select only the attention maps of the CLS token
attention_maps = attention_maps[:, :, 0, 1:]
np_attention_maps = attention_maps.cpu().detach().numpy()

# ------
# ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å¯è¦–åŒ–
def visualize_attention(num_images, images, labels, predictions, attention_maps, output):
    """
    Visualize the attention maps of the first 9 images.
    """
    classes = ('daisy', 'dandelion', 'roses', 'sunflowers', 'tulips')
    # Pick images
    raw_images = [np.asarray(images[i]) for i in range(num_images)]
    labels = [labels[i] for i in range(num_images)]
    # ---
    # Then average the attention maps of the CLS token over all the heads
    attention_maps = attention_maps.mean(dim=1)
    # Reshape the attention maps to a square
    num_patches = attention_maps.size(-1)
    size = int(math.sqrt(num_patches))
    attention_maps = attention_maps.view(-1, size, size)
    # Resize the map to the size of the image
    attention_maps = attention_maps.unsqueeze(1)
    attention_maps = F.interpolate(attention_maps, size=(224, 224), mode='bilinear', align_corners=False)
    attention_maps = attention_maps.squeeze(1)
    # Plot the images and the attention maps
    fig = plt.figure(figsize=(16, 12))
    mask = np.concatenate([np.ones((224, 224)), np.zeros((224, 224))], axis=1)
    for i in range(num_images):
        ax = fig.add_subplot(3, 3, i+1, xticks=[], yticks=[])
        img = np.concatenate((raw_images[i], raw_images[i]), axis=1)
        ax.imshow(img)
        # Mask out the attention map of the left image
        extended_attention_map = np.concatenate((np.zeros((224, 224)), attention_maps[i].cpu()), axis=1)
        extended_attention_map = np.ma.masked_where(mask==1, extended_attention_map)
        ax.imshow(extended_attention_map, alpha=0.5, cmap='jet')
        # Show the ground truth and the prediction
        gt = classes[labels[i]]
        pred = classes[predictions[i]]
        ax.set_title(f"gt: {gt} / pred: {pred}", color=("green" if gt==pred else "red"))
    plt.tight_layout()
    if output is not None:
        plt.savefig(output)
    plt.show()

# ------
num_images = 9

# ------
#@title Visualize Attetion
visualize_attention(num_images, np_images, np_labels, np_predictions, attention_maps, "atten-tion_flower_sono1.png")

```

QEU:FOUNDER ï¼š â€œã“ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒãƒƒãƒ—ä½œè£½ç”¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯ã€åˆã‚ã«ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨å­¦ç¿’æ¸ˆã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™ã€‚ã§ã™ã‹ã‚‰ã€å­¦ç¿’ç”¨ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’å®Ÿè¡Œã—ã€ãã‚Œã‹ã‚‰ç¶šã‘ã¦ãƒãƒƒãƒ—ã‚’ç”Ÿæˆã—ãŸã„å ´åˆã«ã¯å‰åŠã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯ä¸è¦ã§ã™ã€‚**ã ã‹ã‚‰å‰åŠã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®èª¬æ˜ã¯çœç•¥ã—ã¾ã—ãŸ**ã€‚ãã—ã¦ã€è‚å¿ƒã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒãƒƒãƒ—ã®ä½œè£½çµæœã‚’ãƒ‰ãƒ³ï¼â€

![imageJRL8-10-8](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-8.jpg)

Då…ˆç”Ÿ ï¼š â€œã‚„ã£ã±ã‚Šã€224x224ã®ç”»åƒã¯ãã‚Œã„ã ã€‚ã§ã‚‚ã€å…ƒç”»åƒã®è‰²ãŒå°‘ã—å¤‰ã‚ã£ã¦ã„ã¾ã›ã‚“ï¼Ÿâ€

QEU:FOUNDER  ï¼š â€œã“ã‚Œã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ã¨ãã«(0,225)ã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’(0,1)ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã—ãŸã®ã§èµ·ããŸã‚‚ã®ã§ã™ã€‚å…ƒç”»åƒã¯ç‰¹ã«é‡è¦ã§ã‚‚ãªã„ã®ã§ã€ã“ã®ã¾ã¾ã«ã—ã¦ã„ã¾ã™ã€‚â€

**ï¼ˆKeras-Fine-tuning:ãã®ï¼‘ï¼‰**

![imageJRL8-10-9](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-9.jpg)

Céƒ¨é•· : â€œã†ï½ã‚“ãƒ»ãƒ»ãƒ»ã€æ–­å®šã¯ã§ããªã„ã§ã™ã€‚ãƒ»ãƒ»ãƒ»ã§ã‚‚ã€Keras-ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ä½¿ã£ãŸã¨ãã‚ˆã‚Šã‚‚ã€**ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®åˆ†å¸ƒãŒè‰¯ããªã£ã¦ã„ã‚‹**æ°—ãŒã—ã¾ã™ã€‚ã©ã†ã§ã™ã‹ï¼ŸDå…ˆç”Ÿãƒ»ãƒ»ãƒ»ã€‚â€

**ï¼ˆKeras-Fine-tuning:ãã®ï¼’ï¼‰**

![imageJRL8-10-10](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-10.jpg)

Då…ˆç”Ÿ ï¼š â€œåŒæ„ã—ã¾ã™ã€‚ä»Šå›ã¯ã€**ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãŒã€ŒèŠ±ã€ã«å‘ã„ã¦ã„ã¾ã™**ã€‚ãã†ã„ãˆã°ãƒ»ãƒ»ãƒ»ã€ã²ã¨ã¤è³ªå•ãŒã‚ã‚Šã¾ã™ã€‚ä»Šå›ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒãƒƒãƒ—ã‚’ä½œã‚‹é–¢æ•°ã®æ§‹é€ ãŒã‹ãªã‚Šå¤‰ã‚ã£ãŸã®ã¯ãªãœã§ã™ã‹ï¼Ÿæœ¬æ¥ã¯ã€ãã‚“ãªã“ã¨ã‚’ã™ã‚‹å¿…è¦ãŒãªã„ã¯ãšãªã®ã«ãƒ»ãƒ»ãƒ»ã€‚â€

QEU:FOUNDER  ï¼š â€œå°ç”Ÿã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°èƒ½åŠ›ã®é™ç•Œã§ã™ã€‚å‰å›ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ã‚ˆã†ã«å€‹åˆ¥ã®ç”»åƒã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŒ–ã™ã‚‹æ–¹æ³•ã§ã¯ã©ã†ã—ã¦ã‚‚ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—ç”Ÿæˆã®é–¢æ•°ä¸Šã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦å‹•ã‹ãªã‹ã£ãŸã®ã§ã€å­¦ç¿’ã¨åŒæ§˜ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¨ˆç®—ã—ã¦å…¥åŠ›ã™ã‚‹ã‚ˆã†ã«ã—ãŸã‚“ã§ã™ã€‚ã‚‚ã¡ã‚ã‚“ã€ã“ã‚Œã¯è‰¯ã„æ–¹æ³•ã§ã¯ãªã„ãŒã€çš†ã•ã‚“ã§æ”¹é€ ãƒ»ä¿®æ­£ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚â€

Då…ˆç”Ÿ ï¼š â€œè‡ªã‚‰ã®ç„¡èƒ½ã‚’åéœ²ã—ãŸã€ã¨ã¦ã‚‚æ­£ç›´ã§ã‚ˆã‚ã—ã„ãƒ»ãƒ»ãƒ»ï¼ˆç¬‘ï¼‰ã€‚â€

QEU:FOUNDERï¼ˆè¨­å®šå¹´é½¢65æ­³ï¼‰ ï¼š â€œã“ã®ç¨‹åº¦ã®ãƒ‡ã‚¸ã‚¿ãƒ«æŠ€è¡“ã‚’çŸ¥ã£ã¦ã„ã‚‹è‹¥è€…ã¯ä¸–ã®ä¸­ã«ãŸãã•ã‚“ã„ã¾ã™ã€‚ãŸã ã—ã€å½¼ã‚‰ã¯ã“ã®ãƒ†ã‚¯ãƒãƒ­ã‚¸ã‚’ã©ã®ã‚ˆã†ã«ä½¿ã†ã¹ãã‹ï¼ˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã‚’çŸ¥ã‚‰ãªã„ã‚“ã§ã™ã€‚ãã‚Œã‚’**ã€Œæ©‹æ¸¡ã—ã™ã‚‹ä»•äº‹ã€**ã‚’å°ç”ŸãŒè¡Œã£ã¦ã„ã‚‹ã‚ã‘ã§ã™ã€‚â€

### [ï¼å¯„ä»˜ã®ãŠé¡˜ã„(click here)ï¼œ](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

QEU:FOUNDER  ï¼š â€œ**è€éª¨ã«é­æ‰“ã£ã¦ãƒ»ãƒ»ãƒ»ã€‚**â€

## ï½ ã¾ã¨ã‚ ï½

### ãƒ»ãƒ»ãƒ» å‰å›ã®ã¤ã¥ãã§ã™ ãƒ»ãƒ»ãƒ»

Då…ˆç”Ÿ ï¼š â€œé¢¨ã®æ™‚ä»£ãŒå§‹ã¾ã‚Šã€Cå›½ã®å·»ãè¿”ã—ãŒå§‹ã¾ã‚Šã¾ã—ãŸã€‚EVè»Šã ã‘ã˜ã‚ƒãªãã€ãªã‚“ã¨Cå›½è£½ã®ã‚¨ãƒ³ã‚¸ãƒ³è»Šã‚‚å£²ã‚Œã¦ã„ã‚‹ã‚‰ã—ã„ã€‚ä¸€ä½“ã€æµ·å¤–ã§ã¯ä½•ãŒèµ·ãã¦ã„ã‚‹ã‚“ã§ã—ã‚‡ã†ã­ï¼Ÿâ€

![imageJRL8-10-11](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-11.jpg)

QEU:FOUNDER ï¼š â€œã¾ãšã¯ã€ãƒ¢ãƒã®å“è³ªã«å·®ãŒãªããªã£ã¦ã„ã‚‹ã‚“ã§ã—ã‚‡ã†ã€‚ãã‚Œã¯å½“ãŸã‚Šå‰ã ã‚ˆã­ã€‚ã‚‚ã£ã¨é‡è¦ãªã®ã¯ã€ã€ŒMade in PRCã€ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ãŒå¤‰ã‚ã£ãŸã“ã¨ãƒ»ãƒ»ãƒ»ã€‚**EVè»Šã®è²©å£²ã‚’é€šã˜ã¦ã€ï¼ˆCå›½ä¼æ¥­ã¯ï¼‰ä¸€æ°—ã«å…ˆé€²çš„ã ã¨æ€ã‚ã‚Œã‚‹ã‚ˆã†ã«ãªã£ãŸ**ã‚ˆã­ã€‚â€

Då…ˆç”Ÿ ï¼š â€œãã¬ã¬ãƒ»ãƒ»ãƒ»ã€‚ã§ã¯ã€Jå›½ï¼ˆä¼æ¥­ï¼‰ã®å·»ãè¿”ã—ãŒã‚ã‚‹ã‚“ã§ã—ã‚‡ã†ã‹ã­ï¼Ÿâ€

QEU:FOUNDER ï¼š â€œ**ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³ã®è€ƒãˆæ–¹ã‚’å¤‰ãˆã‚Œã°ã€ã‚¤ãƒ¡ãƒ¼ã‚¸ã®å·»ãè¿”ã—ã¯å¯èƒ½ã‹ã‚‚ã—ã‚Œãªã„**ã€‚ã‚µãƒ—ãƒ©ã‚¤ãƒ¤ãƒ¼ã¯ã€éƒ¨å“ã‚’ä¾›çµ¦ã—ã¦ã‚‚ã‚‰ã†ä¼šç¤¾ã§ã¯ãªãã€è‡ªç¤¾è£½å“ã‚’è²·ã£ã¦ã‚‚ã‚‰ã†ã€ŒãŠå®¢æ§˜ãŒã„ã‚‰ã£ã—ã‚ƒã‚‹ã€ä¼šç¤¾ã ã¨æ€ã†ã¨ã„ã„ã¨æ€ã„ã¾ã™ã€‚ã‚µãƒ—ãƒ©ã‚¤ã‚¢ã®ä½œæ¥­å“¡ãŒãƒŸã‚¹ã‚’ã—ãŸã¨ã—ã¦ã‚‚ã€é¡§å®¢ãŒè‡ªã‚‰ãŒè¨­è¨ˆã—ãŸæ¤œæŸ»æ©Ÿã§è¦‹ã¤ã‘ã‚Œã°ã„ã„ã€‚ã“ã‚Œã“ã**ã€Œå“è³ªã®ãƒ¬ã‚¸ãƒªã‚¨ãƒ³ã‚¹ã€**ï¼ï¼ãã®ä½œæ¥­å“¡ã¯æ•‘ã‚ã‚ŒãŸæ„Ÿæ¿€ã®ã‚ã¾ã‚Šã€ä¸€æ°—ã«ãƒ•ã‚¡ãƒ³ã«ãªã£ã¦ã€ãã“ã®å¾“æ¥­å“¡å…¨å“¡ãŒå¤§é‡ã«è£½å“ã‚’è²·ã£ã¦ãã‚Œã‚‹ã§ã—ã‚‡ã†ã€‚**é¢¨ã®æ™‚ä»£(2024ï½)ã«ãŠã„ã¦ã€ã€Œã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³ã¯æ¾å–ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ãªã„ã€**ã‚“ã§ã™ã‚ˆã€‚â€

Då…ˆç”Ÿ ï¼š â€œFOUNDERã®è¨€ã£ã¦ã„ã‚‹ã“ã¨ãŒçªé£›ã™ãã¦ã€æ™®é€šã®äººã¯è¨³ãŒåˆ†ã‹ã‚‰ãªã„ã¨ã‚‚ã„ã¾ã™ã‚ˆã€‚ã¡ãªã¿ã«ã€ç§ã‚‚æœ€åˆã¯ã€Œã‚­ãƒ§ãƒˆãƒ³ãƒ»ãƒ»ãƒ»ã€ã¨ã—ã¾ã—ãŸãƒ»ãƒ»ãƒ»ï¼ˆç¬‘ï¼‰ã€‚â€

![imageJRL8-10-12](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-12.jpg)

QEU:FOUNDER ï¼š â€œã“ã®ç†è«–ã®ãƒã‚¤ãƒ³ãƒˆã¯**ã€Œè¿‘å¹´ã®æ€¥æ¿€ãªæŠ€è¡“é©æ–°ã€**ã§ã™ã€‚ï¼ˆã„ã‚ã‚†ã‚‹ï¼‰æ–°èˆˆå›½ã§ã¯ã€ã€Œè»½å·¥æ¥­â†’é‡å·¥æ¥­â†’é›»å­å·¥æ¥­etcã€ãªã‚“ã‹ã¨ã„ã†ã€ç”£æ¥­é©å‘½ã‹ã‚‰ã®äººé¡ã®ç§‘å­¦æŠ€è¡“ç™ºå±•ã®ã‚†ã£ãã‚Šã—ãŸã‚¹ãƒ†ãƒƒãƒ—ã‚’ãµã¾ãšã«ã€ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã•ã‚ŒãŸãƒ†ã‚¯ãƒãƒ­ã‚¸ã‚’ä½¿ã£ã¦ã€ã„ããªã‚Šç”£æ¥­ãŒæˆé•·ã™ã‚‹ã‚“ã§ã™ã€‚ã ã‹ã‚‰ã€ãã“ã«ä½ã‚€äººãŸã¡ã¯æ˜”ã®ã“ã¨ã¯æ°—ã«ã—ãªã„ã€‚ã‚ªãƒƒã‚µãƒ³ã®ã€**ã€Œæ˜”ã€ä¿ºã¯ã™ã”ã‹ã£ãŸã‚“ã ãï¼ã€**ã¨ã„ã†æ˜”è©±ã¯ã€ã‚‚ã¯ã‚„ã ã‚Œã‚‚èã„ã¦ã„ãªã„ãƒ»ãƒ»ãƒ»ã€‚â€

### ã‚ªãƒƒã‚µãƒ³ï¼š å¾“æ¥­å“¡ã®çš†ã•ã‚“ã«ã¯ãƒ†ãƒ¬ãƒ“ã‚’è¦‹ã¦ãã ã•ã„ã€‚çš†ãŒåŒã˜ã‚ˆã†ã«è€ƒãˆã¦ãã ã•ã„ã€‚
### ã‚ªãƒƒã‚µãƒ³ï¼šâ€œæ¤œæŸ»ä¸æ­£â€ã£ã¦ã„ã†ã®ã¯ãªã‚¡ã€ï¼ˆçµ„ç¹”å¤–ã«ä¸æ­£ã‚’ï¼‰æ¼ã‚‰ã—ãŸãƒ¤ãƒ„ãŒæ‚ªã„ã‚“ã ã‚ˆã€‚
### ã‚ªãƒƒã‚µãƒ³ï¼šç§ã®ä½¿å‘½ã¯ã“ã®ä¼šç¤¾ã§çµ‚èº«é›‡ç”¨åˆ¶ã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã«ã‚ã‚‹ã€‚**(ã‚ã–ã‚ã–201Xå¹´ã§ã€ã‹ã¤æµ·å¤–ã®ä¼šç¤¾ã§ã“ã‚Œã‚’è¨€ã†ã‹ï¼ï¼Ÿ**ãŠã‚ã§ã¨ã†ï¼ã‚ã®å›½ã€ã“ã‚Œã‹ã‚‰ãƒãƒ³ãƒãƒ³ä¼¸ã³ã‚‹ã‚ˆï¼**)

Céƒ¨é•· : â€œãªã‚‹ã»ã©ã€‚é¼»ã‚’ã»ã˜ã£ã¦ã€ã€Œãµï½ã‚“ã€ã¨ã„ã†æ„Ÿã˜ãƒ»ãƒ»ãƒ»ï¼ˆç¬‘ï¼‰ã€‚â€

![imageJRL8-10-13](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-13.jpg)

QEU:FOUNDER ï¼š â€œã ã‹ã‚‰ã€å½¼ã‚‰ã¯å¤ã„æŠ€è¡“ã‚„è£½å“ã«å¯¾ã—ã¦ã‚‚é–¢å¿ƒãŒè–„ã„ã‚ã‘ãƒ»ãƒ»ãƒ»ã€‚ã•ã‚‰ã«ã„ã†ã¨ã€å½¼ã‚‰æ¶ˆè²»è€…ã¯ã€ã§ãã‚Œã°è‡ªåˆ†ã®å›½ã§ä½œã£ãŸã‚‚ã®ã‚’æ¶ˆè²»ã—ãŸã„ã‚ã‘ã€ãã‚Œã¯å½“ãŸã‚Šå‰ã®å¿ƒæƒ…ã ã‚ˆã­ã€‚ã•ã¦ã€ãã®ä¸€æ–¹ã§ãƒ»ãƒ»ãƒ»ã€‚â€

![imageJRL8-10-14](/2024-01-06-QEUR23_VTRANS9/imageJRL8-10-14.jpg)

QEU:FOUNDER ï¼š â€œä»Šã¾ã§ã®è©±ã‚’é€†ã«ã„ã†ã¨ã€ï¼ˆæ–°èˆˆå›½ã®ï¼‰è‹¥è€…ã®å ´åˆã€**ãƒ¢ãƒã‚„ãƒ¡ãƒ¼ã‚«ãƒ¼ã®è‰¯ã—æ‚ªã—ã®è©•ä¾¡ã‚¹ã‚±ãƒ¼ãƒ«ã‚‚æ–°ã—ã„æŠ€è¡“ãŒãƒ™ãƒ¼ã‚¹ã«ãªã‚‹**ã‚ã‘ã§ã™ã€‚ã¡ãªã¿ã«ã€å‰å›ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯Vå›½ã®è‹¥è€…ãŒä½œã£ãŸGithubã®å†…å®¹ãŒãƒ™ãƒ¼ã‚¹ã«ãªã‚Šã¾ã—ãŸã€‚ãã†ã„ã£ãŸå½¼ã‚‰ãŒã€**ã€Œæœªæ¥ã®æ¶ˆè²»è€…ã€**ã¨ãªã‚‹ã‚ã‘ã§ã™ãƒ»ãƒ»ãƒ»ã€‚â€

Då…ˆç”Ÿ ï¼š â€œãã‚Œã¯ãƒ­ãƒ¼ãƒˆãƒ«å…±ã«ã¯å³ã—ã„ãƒ»ãƒ»ãƒ»ã€‚ãã†ã„ã†äººãŸã¡ã«ã‚¢ãƒ”ãƒ¼ãƒ«ã™ã‚‹ã«ã¯ï¼Ÿâ€

[![MOVIE1](http://img.youtube.com/vi/e8aiitAAAZY/0.jpg)](http://www.youtube.com/watch?v=e8aiitAAAZY "æ€’ï¼åšé¡”ç„¡æ¥ï¼†ç„¡çŸ¥ã®è±Šç”°ä¼šé•·ã®ç™ºè¨€ã€‚ã€Œãªãœæ—¥æœ¬äººã®çµ¦æ–™ãŒä¸ŠãŒã‚‰ãªã„ã®ã‹ã€ã«ã¤ã„ã¦è‡ªå·¥ä¼šã®è±Šç”°ç« ç”·ä¼šé•·ãŒä»°å¤©ç™ºè¨€ã€‚çµŒæ¸ˆç•Œã®ãƒœã‚¹ã¯ç«¹ä¸­å¹³è”µã¨åŒãƒ¬ãƒ™ãƒ«ã‹ã€‚")

QEU:FOUNDER ï¼š â€œã“ã‚“ãªã‚¢ãƒ”ãƒ¼ãƒ«ï¼ˆâ†‘ï¼‰ã¯ã„ã‹ãŒï¼Ÿâ€

Då…ˆç”Ÿ ï¼š â€œã‚·ãƒã‹ã‚Œã¾ã™ã‚ˆã€‚è‹¥è€…ã«ãƒ»ãƒ»ãƒ»ã€‚â€
