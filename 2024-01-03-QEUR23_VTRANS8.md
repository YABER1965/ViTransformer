---
title: QEUR23_VTRANS8: [PyTorch]CIPHAR10の画像を「直接に」学習する
date: 2024-01-03
tags: ["QEUシステム", "メトリックス", "Python言語", "Vision Transformer", "LLM", "データセット", "Fine-tuning", "イノベーション"]
excerpt: Vision Transformer(ViT)をやってみる
---

## QEUR23_VTRANS8: [PyTorch]CIPHAR10の画像を「直接に」学習する

## ～ Webの世界にいると、すばらしい「お宝」も見つかるもんだ ～

QEU:FOUNDER ： “それでは、徐々に「シフトアップ」します。今回は、PyTorch(HuggingFace-Transformer)を使って予測モデルを構築し、アテンションマップを作成しましょう。。”

D先生 ： “なぜPyTorchなの？以前は、FOUNDERも「Kerasを使うとらくでいいわぁ」と言っていたじゃないですか。この際ですから、Tensorflow(Keras)とPyTorch(Transformer)の比較をしてもらえませんか？”

QEU:FOUNDER ： “品質も、速さも、システムが使うメモリ量にしても、両者にはそれほどの差はないと思いますよ。ただし、両者を細かく比較した資料をみたことはないけど・・・・。でもね、我々としてはPyTorchが圧倒的にいいです。なにしろ、HuggingFaceを使えるからね。”

![imageJRL8-9-1](/2024-01-03-QEUR23_VTRANS8/imageJRL8-9-1.jpg)

C部長 : “自分が作ったデータセットやモデルをアップロードしてシェアしたり、他の人のモノ（モデル・成果物）を使ってファインチューニングができます。確かに、それはとても便利です。じゃあ、**HuggingFace Transformerで開発しましょう**！”

![imageJRL8-9-2](/2024-01-03-QEUR23_VTRANS8/imageJRL8-9-2.jpg)

QEU:FOUNDER ： “おっと、**V国の人がもう開発していました**。名前を見てね・・・（笑）。もう、これは小生がイメージした「理想」にかなり近い状態です。”

C部長 : “じゃあ、これは即、ボクにも使えるん？このJupyter notebookにコピペで使えばいいの？”

D先生 ： “notebookの様子を見た感じであれば、インポートで参照している外部ファイルがあります。それを注意すれば何とかなるんじゃないんですかねえ・・・。”

QEU:FOUNDER ： “手直しもほとんどないので、自分で使って遊んでみればいいです。小生としては、アテンション・マップに注目しているので、以下のようにプログラムを改造しました。ちなみに、あえて省略した部分が多いので、元のgithubを参照してください。”

```python
##############################
# ATTENTON MAP(INCLUDING LOAD MODEL)
##############################
import math
import torch
from torch import nn

# ------
class NewGELUActivation(nn.Module):
    """
    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see
    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415

    Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py
    """

    def forward(self, input):
        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))

# ------
class PatchEmbeddings(nn.Module):
    """
    Convert the image into patches and then project them into a vector space.
    """

    省略

# ------
class Embeddings(nn.Module):
    """
    Combine the patch embeddings with the class token and position embeddings.
    """

    省略

# ------
class AttentionHead(nn.Module):
    """
    A single attention head.
    This module is used in the MultiHeadAttention module.
    """
    省略

# ------
class MultiHeadAttention(nn.Module):
    """
    Multi-head attention module.
    This module is used in the TransformerEncoder module.
    """
	省略
    
# ------
class FasterMultiHeadAttention(nn.Module):
    """
    Multi-head attention module with some optimizations.
    All the heads are processed simultaneously with merged query, key, and value projections.
    """
	省略

# ------
class MLP(nn.Module):
    """
    A multi-layer perceptron module.
    """
    省略
	
# ------
class Block(nn.Module):
    """
    A single transformer block.
    """
    省略

# ------
class Encoder(nn.Module):
    """
    The transformer encoder module.
    """
    省略

# ------
class ViTForClassfication(nn.Module):
    """
    The ViT model for classification.
    """
    省略
     
# ------
import json, os, math
import matplotlib.pyplot as plt
import numpy as np
import torch
from torch.nn import functional as F
import torchvision
import torchvision.transforms as transforms

# -----
def load_experiment(experiment_name, model):
    # ---
    checkpoint_name="model_final.pt", 
    base_dir="experiments"
    outdir = os.path.join(base_dir, experiment_name)
    # Load the config
    configfile = os.path.join(outdir, 'config.json')
    with open(configfile, 'r') as f:
        config = json.load(f)
    # Load the metrics
    jsonfile = os.path.join(outdir, 'metrics.json')
    with open(jsonfile, 'r') as f:
        data = json.load(f)
    train_losses = data['train_losses']
    test_losses = data['test_losses']
    accuracies = data['accuracies']
    # Load the model
    #model = ViTForClassfication(config)
    #cpfile = os.path.join(outdir, checkpoint_name)
    cpfile = "./experiments/vit-with-50-epochs/model_final.pt"
    model.load_state_dict(torch.load(cpfile))
    return config, model, train_losses, test_losses, accuracies

# -----
import torch
from torch import nn, optim

device = "cuda" if torch.cuda.is_available() else "cpu"

config = {
    "patch_size": 4,  # Input image size: 32x32 -> 8x8 patches
    "hidden_size": 48,
    "num_hidden_layers": 4,
    "num_attention_heads": 4,
    "intermediate_size": 4 * 48, # 4 * hidden_size
    "hidden_dropout_prob": 0.0,
    "attention_probs_dropout_prob": 0.0,
    "initializer_range": 0.02,
    "image_size": 32,
    "num_classes": 10, # num_classes of CIFAR10
    "num_channels": 3,
    "qkv_bias": True,
    "use_faster_attention": True,
}

# ------
# Create the model, optimizer, loss function and trainer
model = ViTForClassfication(config)
print(model)

```

QEU:FOUNDER ： “まずは**ViTモデルの構造**をつくりました。”

![imageJRL8-9-3](/2024-01-03-QEUR23_VTRANS8/imageJRL8-9-3.jpg)

C部長 : “どんなモデルなんですか？”

D先生 ： “知らん・・・（笑）。ちゃんと学習できてるから、「いいモデル」だと思うよ。”

QEU:FOUNDER ： “まあ、G社がプレ・トレーニング用として公開しているモデルよりも簡単なんじゃない？**モデルが簡単だと、少ない情報で学習できる。その逆も真・・・。**つづきにいきましょう。”


```python
# -----
# Load model
experiment_name = 'vit-with-50-epochs' #@param {type:"string"}
# ---
config, model, train_losses, test_losses, accuracies = load_experiment(experiment_name, model)
print(config)
print(model)
print(train_losses)
print(test_losses)
#print(accuracies)
```

C部長 : “あれあれ？ここでも、またモデルを読み込んだ？”

QEU:FOUNDER  ： “以前に50エポック分だけ学習した**モデルの重み**を読み込んでいます。「気合（時間と資源）」を入れれば、もうちょっと性能が上がったかな？”

![imageJRL8-9-4](/2024-01-03-QEUR23_VTRANS8/imageJRL8-9-4.jpg)

C部長 : “まあ、70％の精度が出ていれば大したものですよ。テスト段階なのですから・・・。”

![imageJRL8-9-5](/2024-01-03-QEUR23_VTRANS8/imageJRL8-9-5.jpg)

D先生 ： “モデルと同時に、いろいろな学習結果を読み込んでいますね。”

QEU:FOUNDER ： “このプログラムは便利でしょう？それでは、いよいよ**アテンション・マップ**を作成しましょう。”

```python
# -----
@torch.no_grad()
def visualize_attention(model, output=None, device="cuda"):
    """
    Visualize the attention maps of the first 4 images.
    """
    model.eval()
    # Load random images
    num_images = 16
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)
    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
    # Pick 30 samples randomly
    indices = torch.randperm(len(testset))[:num_images]
    raw_images = [np.asarray(testset[i][0]) for i in indices]
    labels = [testset[i][1] for i in indices]
    # Convert the images to tensors
    test_transform = transforms.Compose(
        [transforms.ToTensor(),
        transforms.Resize((32, 32)),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
    images = torch.stack([test_transform(image) for image in raw_images])
    # Move the images to the device
    images = images.to(device)
    model = model.to(device)
    # Get the attention maps from the last block
    logits, attention_maps = model(images, output_attentions=True)
    # Get the predictions
    predictions = torch.argmax(logits, dim=1)
    # Concatenate the attention maps from all blocks
    attention_maps = torch.cat(attention_maps, dim=1)
    # select only the attention maps of the CLS token
    attention_maps = attention_maps[:, :, 0, 1:]
    # ---
    print("dimensions:",attention_maps.shape)
    # ---
    # Then average the attention maps of the CLS token over all the heads
    attention_maps = attention_maps.mean(dim=1)
    # Reshape the attention maps to a square
    num_patches = attention_maps.size(-1)
    size = int(math.sqrt(num_patches))
    attention_maps = attention_maps.view(-1, size, size)
    # Resize the map to the size of the image
    attention_maps = attention_maps.unsqueeze(1)
    attention_maps = F.interpolate(attention_maps, size=(32, 32), mode='bilinear', align_corners=False)
    attention_maps = attention_maps.squeeze(1)
    # Plot the images and the attention maps
    fig = plt.figure(figsize=(20, 10))
    mask = np.concatenate([np.ones((32, 32)), np.zeros((32, 32))], axis=1)
    for i in range(num_images):
        ax = fig.add_subplot(4, 4, i+1, xticks=[], yticks=[])
        img = np.concatenate((raw_images[i], raw_images[i]), axis=1)
        ax.imshow(img)
        # Mask out the attention map of the left image
        extended_attention_map = np.concatenate((np.zeros((32, 32)), attention_maps[i].cpu()), axis=1)
        extended_attention_map = np.ma.masked_where(mask==1, extended_attention_map)
        ax.imshow(extended_attention_map, alpha=0.5, cmap='jet')
        # Show the ground truth and the prediction
        gt = classes[labels[i]]
        pred = classes[predictions[i]]
        ax.set_title(f"gt: {gt} / pred: {pred}", color=("green" if gt==pred else "red"))
    if output is not None:
        plt.savefig(output)
    plt.show()

# ------
#@title Visualize Attetion
visualize_attention(model, "attention_tn_sono2.png")

```

QEU:FOUNDER ： “これが今回の結果です。よくできているでしょう？”

![imageJRL8-9-6](/2024-01-03-QEUR23_VTRANS8/imageJRL8-9-6.jpg)

D先生 ： “いやぁ・・・。キレイですね。”

QEU:FOUNDER  ： “Dセンセ・・・、他にコメントはないの？”

C部長 : “**前回はレイヤ別にアテンション・マップを出しました**よね。あれ、やらないんですか？？すごく面白かったのに・・・。”

**（その３:車）**

![imageJRL8-9-7](/2024-01-03-QEUR23_VTRANS8/imageJRL8-9-7.jpg)

**（その５:車）**

![imageJRL8-9-8](/2024-01-03-QEUR23_VTRANS8/imageJRL8-9-8.jpg)

D先生 ： “しまった・・・。改めてプログラムを見ると、**全てのレイヤのアテンション値の平均**を取っていますよね。”

QEU:FOUNDER  ： “もちろん、本プロジェクトの後半でアテンション毎のマップに切り替えますよ。”

C部長 : “う～ん・・・。初めの話を蒸し返して申し訳ないです。Fine-tuningにしたらダメなんですかねえ・・・。あれだけうまく行っているんだし・・・。”

![imageJRL8-9-9](/2024-01-03-QEUR23_VTRANS8/imageJRL8-9-9.jpg)

D先生 ： “我々としては、異常検出だけでなく、**位置特定をしたい**。ただし、いろいろ他にもやり方もあるんでしょうね。類似の目的の論文も、すでにあることだし・・・。”

**Abstract:**
We present a transformer-based image anomaly detection and localization network. Our pro-posed model is a combination of a reconstruction-based approach and patch embedding. The use of trans-former networks helps preserving the spatial information of the embedded patches, which is later pro-cessed by a Gaussian mixture density network to localize the anomalous areas. In addition, we also publish BTAD, a real-world industrial anomaly dataset. Our results are compared with other state-of-the-art algo-rithms using publicly available datasets like MNIST and MVTec. Index Terms—Anomaly Detection, Anomaly segmentation, Vision transformer, Gaussian density approximation, Anomaly dataset
**要約:**
トランスフォーマー・ベースの画像異常検出および位置特定ネットワークを紹介します。 私たちが提案するモデルは、再構成ベースのアプローチとパッチ埋め込みを組み合わせたものです。 トランスフォーマー ネットワークの使用は、埋め込まれたパッチの空間情報を保存するのに役立ちます。この情報は後でガウス混合密度ネットワークによって処理され、異常領域の位置が特定されます。 さらに、実際の産業異常データセットである BTAD も公開しています。 私たちの結果は、MNIST や MVTec などの公的に利用可能なデータセットを使用した他の最先端のアルゴリズムと比較されます。 索引用語 - 異常検出、異常セグメンテーション、ビジョン トランスフォーマー、ガウス密度近似、異常データセット

C部長 : “さすがに、進んでいる海外では、すでに（異常検出技術の開発を）やっているんですね。”

QEU:FOUNDER  ： “さっきのFine-tuningの件に戻るが、小生の結論からいうと**「やってみないとわからん」**です。”

D先生 ： “・・・あの件・・・。まだ始めていませんからね。”

QEU:FOUNDER ： “そうですね。始めるのはまだ早い。一歩一歩、開発をつづけましょう。”

### [＞寄付のお願い(click here)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

QEU:FOUNDER  ： “寄付をください。開発にはお金がかかりますからね。”


## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

D先生 ： “うわあ・・・、なつかしい・・・。E.デミング博士がモデルにしたのは、**彼が当時に見た「J国企業の姿」だった**んですよね！”

### オッサン： 従業員の皆さんにはテレビを見てください。皆が同じように考えてください。
### オッサン：“検査不正”っていうのはなァ、（組織外に不正を）漏らしたヤツが悪いんだよ。
### オッサン：私の使命はこの会社で終身雇用制を実現することにある。***(わざわざ201X年で、かつ海外の会社でこれを言うか！？)***

QEU:FOUNDER ： “あ～あ・・・、時が過ぎ、J国も**劣化したもんだ**。ただし、検査に対する見方、PPM品質時代における統計的手法の有用性など、彼の14原則にも修正が必要となる部分はたくさんある。”

- 競争力を保つため、製品やサービスの向上を常に心がける環境を作る。
- 最高経営者がその責任者を決める。
- 新しい哲学を採用する。我々は新たな経済時代にいる。遅延、間違い、材料の欠陥、作業の欠陥などの一般常識となっている水準には満足できない。
- 全品検査への依存を止める。品質は統計的手法で向上させる（完成後に欠陥を見つけるのではなく、欠陥を防止せよ）。
- 価格だけに基づいて業者を選定することを止める。価格と品質によって選定する。統計的手法に基づく品質保証のできない業者は排除していく。
- 問題を見逃さない。
- 全体（設計、受け入れ材料、製造、保守、改良、トレーニング、監視、再教育）を継続的に向上させるのがマネジメントの役割である。
- 職場教育（OJTなど）の手法を導入し、義務付ける。
- 職場のリーダーは単に数値ではなく品質で評価せよ。それによって自動的に生産性も向上する。
- マネジメントは、職場のリーダーから様々な障害（固有の欠陥、保守不足の機械、貧弱なツール、あいまいな作業定義など）について報告を受けたら、迅速に対応できるよう準備しておかなければならない。
- 社員全員が会社のために効果的に作業できるよう、不安を取り除く。
- 部門間の障壁を取り除く。研究、設計、販売、製造の各部門の人々は様々な問題に一丸となって対応しなければならない。
- 数値目標を排除する。新たな手法も提供せずに生産性の向上だけをノルマとしない。数値割り当てを規定する作業標準を作業者やマネジメントから排除する。
- 時間給作業員から技量のプライドを奪わない。とりわけ年次・長所によって評価することや目標による管理は廃止する。
- 強健な教育プログラムを実施する。全員で変革に取り組む。
- 最高経営陣の中で、上記13ポイントを徹底させる構造を構築する。

C部長 : “今、本当に再評価が必要なのは、彼の言葉なのでしょう。”

QEU:FOUNDER ： “**もっとサプライヤを大切にしなきゃ・・・。**インダストリー5.0の時代において、品質保証に責任があるのは顧客です。サプライアの責任はコストだけですよ。多分、もうすぐ**「風の時代」**にむけて品質に対する見方が大きく変わってきますよ。”

![imageJRL8-9-10](/2024-01-03-QEUR23_VTRANS8/imageJRL8-9-10.jpg)

QEU:FOUNDER ： “品質という言葉が消えてしまったサプライチェーンにおいて、顧客とサプライア（組織と請負先）の付き合い方も、**「コミュニケーション重視」**に変わってくるしかないです。サプライアの品質保証が顧客責任であるならばね・・・。”

![imageJRL8-9-11](/2024-01-03-QEUR23_VTRANS8/imageJRL8-9-11.jpg)

D先生 ： “巷をみると、すでに風の時代が始まりとともに**「C国（企業）の巻き返し」**が始まりました。EV車だけじゃなく、なんとエンジン車も売れているらしい。一体、海外では何が起きているんでしょうね？”

QEU:FOUNDER ： “まずは、**モノの品質に差がなくなっている**んでしょう。それは当たり前だよね。もっと重要なのは、「Made in PRC」のイメージが変わったこと・・・。EV車で一気に先進的な企業だと思われるようになったよね。”

D先生 ： “ぐぬぬ・・・。では、J国(企業)の巻き返しがあるんでしょうかね？”

QEU:FOUNDER ： “さあ・・・、笑うしかない・・・（笑）。ただし、サプライチェーンの考え方を変えれば、イメージの巻き返しは可能かもしれない。サプライヤーは部品を供給してもらう会社ではなく、自社製品を買ってもらう**「お客様がいらっしゃる」**会社だと思うといいと思います。”

C部長 : “理屈としてはアリでしょう。でも、サプライアはミスをします。迷惑な人たちでもあります。”

QEU:FOUNDER ： “ミスをしたとしても、**顧客が自らが設計した検査機で見つければいい**。これこそ「品質のレジリエンス」！！サプライヤーは救われた感激のあまり、一気にファンになって、そこの従業員全員が大量に製品を買ってくれるでしょう。”

C部長 : “彼らの親戚や友人、SNSにも教えちゃうでしょうね。”

QEU:FOUNDER ： “もういちど言うけど、もっとサプライヤーを大切にしなきゃ・・・。**風の時代においては、もはや「サプライチェーンは搾取するシステムではない」んですよ。**”
