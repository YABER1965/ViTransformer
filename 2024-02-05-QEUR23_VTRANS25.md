---
title: Technical summary ~ Automatic visual inspection machine using SOART3 metrics and ViT (double mode)
date: 2024-02-05
tags: ["QEUシステム", "メトリックス", "Python言語", "Vision Transformer", "LLM", "データセット", "Fine-tuning", "イノベーション"]
excerpt: Vision Transformer(ViT)をやってみる
---

### Technical summary ~ Automatic visual inspection machine using SOART3 metrics and ViT (double mode)

### Name of invention 
Automatic visual inspection machine using SOART3 metrics and ViT

### Technology field
**【０００１】**
The present invention relates to an automatic computer-based visual inspection machine.

### Technology background
**【０００２】**
Many machine learning logics for image classification have already been proposed, and have achieved great results (Fig. 1). Among these, CNN (Convolutional Neural Network) has achieved high discrimination ability.

**(Fig. 1: History of image recognition competitions)**

![imageJRL8-21-1](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-1.jpg)

**【０００3】**
However, if these techniques that have been successful in image discrimination (Fig. 2) are used directly to detect abnormalities through visual inspection, good results may not necessarily be obtained. On the other hand, a technology called Vision Transformer (ViT) has recently been proposed, and this method may be better than CNN for visual inspection. CNN is excellent at identifying individuals because it creates a feature vector that aggregates information through repeated convolutions (Fig. 3). However, CNN does not have good discrimination accuracy when features are spatially separated (Fig. 4). On the other hand, since ViT is a technology derived from natural language processing, there are no such limitations. In some cases, these results have been published as academic papers (Fig. 5)..

**(Fig. 2: Image recognition example)**

![imageJRL8-21-2](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-2.jpg)

**(Fig. 3: Structure of convolutional neural network)**

![imageJRL8-21-3](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-3.jpg)

**(Fig. 4: Vision Transformer detection capability)**

![imageJRL8-21-4](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-4.jpg)

**(Fig. 5: Examples of academic results)**

![imageJRL8-21-5](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-5.jpg)

**【０００４】**
In addition to the above-mentioned CNN and ViT, many computer-based anomaly detection methods have been proposed, and are being put into practical use mainly for products with extremely high transferability, such as for quality control of printed circuit boards. Typical methods are listed below.

- Image subtraction method
- How to measure distance
- Supervised learning (deep learning)
*Autoencoder (VAE) is considered an intermediate method between deep learning and image difference method.

### Problems to be solved by this invention.

**【０００５】**
It is possible to some extent to perform visual inspections using only CNN or ViT. For example, in the agricultural problem below (Fig. 6), the withering of leaves in some images is considered abnormal (Fig. 7).

**(Fig. 6: Agriculture example of anomaly detection)**

![imageJRL8-21-6](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-6.jpg)

**(Fig. 7: Learning image of bean leaf)**

![imageJRL8-21-7](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-7.jpg)

**【０００６】**
In the above example of disease identification on bean leaves, the ViT model is used to detect abnormalities (diseases). Furthermore, in the case of ViT, an attention map can be generated from the information of the learned model, and the location of anomalies can be discovered (Fig. 8).

**(Fig. 8: Leaf anomaly detection, attention map)**

![imageJRL8-21-8](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-8.jpg)

**【０００７】**
However, when applied to actual manufacturing sites, it is difficult to directly apply CNN and ViT individual discrimination technology to abnormality detection. Images obtained during visual inspection of actual manufacturing processes do not have the same diversity as in the leaf inspection example described above, and the shapes of the products are almost the same except for defective parts. Furthermore, when inspecting individual parts, the entire product has the same color (Fig. 9).

**(Fig. 9: Manufacturing site example, plastic injection molded product)**

![imageJRL8-21-9](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-9.jpg)

**【０００８】**
In addition, product defects, even minute ones, often result in a defective product, and many of them can only be detected using high-resolution (ex 1200x1200) images. However, in the case of methods that form learning models and make predictions, such as CNN and ViT, large images require greater training data preparation and computational costs to mature the model. For example, ViT models often use small images such as 224x224, but images of this size are not suitable for visual inspection. Conversely, this requirement for input image size is also the reason why techniques that use images as they are, such as image subtraction (VAE), are still used in visual inspection.

### Means to solve problems.

**【０００９】**
In this invention, we use ViT and SOART3 synthetic image processing together to solve the above problems. SOART3 is an extension of the Taguchi method technology called RT (Recognition Taguchi) method. The RT method compares a standard vector and a measured vector and generates two metrics: Y1 (sensitivity: β) and Y2 (SN ratio: η) (Fig. 10).

**(Fig. 10: Principle of RT method)**

![imageJRL8-21-10](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-10.jpg)

**【００１０】**
The SOART3 method is a method that extends the RT method that outputs 2D metrics to 3D metrics output and outputs an RGB image. The RT method uses the Eugridian distance, which is a comparison of the standard vector and the measured vector after sensitivity correction, as the signal-to-noise ratio. Whereas, we generalized the distance between two vectors and considered the Minkowski distance (Fig. 11). This distance can be changed from Manhattan distance (p=0) to Chebyshev distance (p=∞) by changing the p value of the parameter. In other words, the SOART3 method uses Manhattan distance and Chebyshev distance instead of Eugrit distance because we want to separate them into two independent metrics and make them three-dimensional (Fig. 12).

**(Fig. 11: Definition of Minkowski distance)**

![imageJRL8-21-11](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-11.jpg)

**(Fig. 12: Concept of SOART3 method)**

![imageJRL8-21-12](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-12.jpg)

**【００１１】**
Here is an example of the SOART3 method described in a Python program. First, calculate the sensitivity (β) from the standard vector (x) and measurement vector (y) data. We then calculate the Chebyshev distance and Manhattan distance between x and the sensitivity-corrected measurement vector βy, respectively. The Chebyshev distance and the Manhattan distance are generated from the same vector (x, βy), and there seems to be a slight correlation between the two distances, so the Chebyshev distance is logarithmically transformed and then the difference from the Manhattan distance is calculated. I'm taking it. Logarithmic transformation is applied to linearize the value behavior and reduce the size of the learning model.

```python
# soaRT3メトリックスを計算する
def calc_soaRT3(tsr_sig_array, tsr_tani_array): 

    # データの抽出
    y = tsr_sig_array
    x = tsr_tani_array
    #print(y)
    #print(x)

    # 感度(β)を計測
    xx = np.dot(x,x) + 0.0001
    xy = np.dot(x,y) + 0.0001
    beta = xy/xx

    # チェビシェフ距離を計測
    vDistance = chebyshev(y,beta*x)

    # マンハッタン距離を計測
    mDistance = np.linalg.norm(y - beta*x, ord=1)
    #print("mDistance: ", mDistance.item())
    
    # 値の対数変換
    log_beta  = math.log(beta)
    log_yita = math.log(mDistance+1.0)
    log_gamma = math.log(vDistance+1.0) - log_yita
    
    return log_beta, log_yita, log_gamma

```

**【００１２】**
Additionally, the SOART3 method can be used in single mode (one camera) or double mode (three cameras) (Fig. 13). Single mode is effective for visual inspection of printed circuit boards and multicolored printed matter, and double mode is effective for inspection of three-dimensional products. In this invention, only double mode will be discussed.

**(Fig. 13: Double mode concept)**

![imageJRL8-21-13](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-13.jpg)

**【００１3】**
The ability to measure three-dimensionality in double mode is based on the same principle that humans (animals) use to measure distance from multiple images (Fig. 14).

**(Fig. 14: Principle of obtaining three-dimensional effect in double mode)**

![imageJRL8-21-14](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-14.jpg)

**【００１４】**
Whereas, we will create a composite image using the SOART3 (double mode) calculation method. First, a central camera takes multiple product images, converts the original RGB values to grayscale, and generates an average image of them (Fig. 15).

**(Fig. 15: Standard data for round terminals, average image)**

![imageJRL8-21-15](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-15.jpg)

**【００１５】**
After that, the left and right images of the inspected item are simultaneously captured and converted to grayscale in the same way as standard images (Fig. 16 and 17).

**(Fig. 16: Left camera image)**

![imageJRL8-21-16](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-16.jpg)

**(Fig. 17: Right camera image)**

![imageJRL8-21-17](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-17.jpg)

**【００１６】**
Processing these three image information (vectors) will give you SOART3 metrics. Double mode takes the difference between the SOART3 metrics of the right and left cameras. As a result, a SOART3 composite image like the one below can be generated (Fig. 18).

**(Fig. 18: SOART3 composite image)**

![imageJRL8-21-18](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-18.jpg)

### Example
**【００１７】**
In this example, we will introduce an application example for visual inspection of connector terminals of wire harnesses. A wire harness is a component that brings together a group of wires that connect multiple parts in an electrical product (Fig. 19), and we will develop a device to visually inspect the condition of the terminals (pins) of its male connector (Fig. 20).

**(Fig. 19: What is a wire harness?)**

![imageJRL8-21-19](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-19.jpg)

**(Fig. 20: Connector terminal inspection)**

![imageJRL8-21-20](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-20.jpg)

**【００１８】**
In the present invention, ViT is used to determine pin abnormalities within the connector. In this case, a relatively small model was generated (Fig. 21).

**(Fig. 21: ViT model used in this experiment)**

![imageJRL8-21-21](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-21.jpg)

**【００１８】**
To create the SOART3 composite image, we created a connector in the virtual space of 3CG software (Blender) and installed the right camera, center camera, and left camera (Fig. 22). The failure modes to be detected are vertical dropout of the round terminal pin (cylindrical) and pin inclination (Fig. 23). Of course, we assume that pin failures occur at each pin location within the connector. At the stage of learning this ViT model, there is only one defective location.

**(Fig. 22: Left-center-right camera images)**

![imageJRL8-21-22](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-22.jpg)

**(Fig. 23: Definition of failure mode)**

![imageJRL8-21-23](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-23.jpg)

**【００１９】**
First, we first learn data only for connectors with round terminals. The normal and defective connectors were rotated, shifted, and the light source energy fluctuated to take pictures (Fig. 24). In addition, we used not only SOART3 composite images but also captured images as training data. This will increase the amount of training data. When learning data from this single type of terminal, we observed a phenomenon in which ACCURACY (discrimination accuracy) changed slowly during the learning process (Fig. 25).

**(Fig. 24: Training data: including left and right raw images and composite images)**

![imageJRL8-21-24](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-24.jpg)

**(Fig. 25: ViT learning progress)**

![imageJRL8-21-25](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-25.jpg)

**【００２０】**
Whereas, as a measure to further accelerate learning, data from a rectangular CG model is added as another terminal type (Fig. 26). In this case, as in the case of the cylindrical terminal described above, an average image of the prism is created and SOART3 processing is performed (Fig. 27).

**(Fig. 26: Prismatic CG model)**

![imageJRL8-21-26](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-26.jpg)

**(Fig. 27: SOART3 composite image of square terminal)**

![imageJRL8-21-27](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-27.jpg)


**【００２１】**
When training was performed using a combination of round terminal and square images, the EPOCH at which Accuracy began to fluctuate became earlier, and the learning speed was faster than with a single type (Fig. 28). In other words, by increasing the number of terminal types that ViT learns, it seems that the ViT model understands that terminal disconnection is the movement and rotation of shapes (cylinders and prisms), regardless of the shape of the terminal.

**(Fig. 28: Learning progress using a combination of round and square terminals)**

![imageJRL8-21-28](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-28.jpg)

**【００２２】**
Now, let's compare the changes in the attention map as the learning of the ViT model progresses (Fig. 29 and 30). At the beginning of learning, the locations with high attention were distributed in addition to the defect location. As learning progresses, when the image is defective, attention will be focused on the location where the defect exists. On the other hand, for connector images with no defects, the attention distribution varies widely.

**(Fig. 29: Early stage of learning ~ EPOCH:200)**

![imageJRL8-21-29](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-29.jpg)

**(Fig. 30: Learning completion stage ~ EPOCH:600)**

![imageJRL8-21-30](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-30.jpg)

**【００２3】**
Using the trained model described above, we will introduce the results of generating an attention map when there are defects in two locations in an inspection image. Even in this case, attention is high at the two locations where the defect is present (Fig. 31 and 32).

**(Fig. 31: Inspection image with defects in two locations)**

![imageJRL8-21-31](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-31.jpg)

**(Fig. 32: Attention map with defects in two locations)**

![imageJRL8-21-32](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-32.jpg)

**【００２４】**
In this way, visual inspection using SOART3+ViT is considered to have extremely high potential for visual inspection applications. In addition, the present invention can discover types of defects that cannot be detected using conventional discrimination models. If the terminal is incorrect (for example, a round terminal was supposed to be used, but a square terminal was actually used), the normal discrimination model will only judge it as a good product. However, since SOART3 processes the standard image by comparing it with the inspection image, it becomes clear that the terminal has a foreign defect (Fig. 33 and 34). For example, in the defect image below, there is a shadow of a round image inside the square terminal, indicating that there is a defective product.

**(Fig. 33: Image of foreign terminal, part 1)**

![imageJRL8-21-33](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-33.jpg)

**(Fig. 34: Image of foreign terminal, part 2)**

![imageJRL8-21-34](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-34.jpg)

**【００２５】**
However, since it uses the ViT model, it requires a larger amount of data for training than the CNN model. Therefore, it is not cost-effective to prepare such a large amount of data for the visual inspection of just one connector. Therefore, it seems essential to build a system for fine tuning that applies deep learning transfer learning (Fig. 35). Furthermore, by performing learning mainly on CG images during pre-training and learning on-site images during fine-tuning, the cost of preparing training data can be reduced.

**(Fig. 35: Applying fine tuning)**

![imageJRL8-21-35](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-35.jpg)


### Industrial applicability

**【００２６】**
When the SOART3 processing method is applied to the ViT model, which has recently received high praise for image classification, the strengths of the SOART3 composite image can complement the weaknesses of the ViT model. This time, I introduced double mode. The comparison with single mode is as follows, and their characteristics are contradictory.

**(Advantages and features of double mode)**

- Excellent for detecting images of three-dimensional objects.
- Requires 3 cameras (offline) and 2 cameras (online)

**(Advantages and features of single mode)**

- Excellent for detecting color images of assembled finished products and printed matter.
- One camera (online) is required. However, it is desirable to have many cameras when offline.

**(Common advantages of double and single)**

- Inspection accuracy is high even for inspection images of products with a single color.
- By processing a high-resolution image (e.g. 1000x1000) using the SOART3 method, it is possible to convert it to a lower resolution image (e.g. 224x224) without reducing the amount of information.
- Since it is a synthetic image, the content that the ViT model learns does not change whether the input is a CG image or an actual image.

**【００２７】**
We will discuss the application of single-mode SOART3 composite images another time.

## ～ まとめ ～

### ・・・ さらに、発明の補足です ・・・

QEU:FOUNDER: “SOART3法について、少し補足しておきたい。第3のメトリックスでチェビシェフ距離を使うというのは、**必ずしも「絶対」ではない**。”

**（再掲図12:SOART3法の考え方）**

![imageJRL8-21-12](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-12.jpg)

C部長: “RT法でユークリッド距離を使っていたものを、マンハッタンとチェビシェフの両極端に分離させたというのは、それなりにわかりやすいですが、これってダメなんですか？”

QEU:FOUNDER: “RT法は画像分析を念頭にして開発されています。ただし、RT法には非常にまれな使い方として、**「非常に少ないメンバー数でも手軽に解析できる判別手法」**という側面があります。このような使い方の場合にはあまりチェビシェフ距離を推奨しません。”

**（RT法の単位空間の設定）**

![imageJRL8-21-37](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-37.jpg)

QEU:FOUNDER: “例えば、以前、「J国の47都道府県において、パフォーマンスが異常な県を検出する」というプロジェクトをしましたよね。この場合には、異常検出のためのメンバーがたった47しかありません。さらに、このとき単位空間を平均的な広島県と静岡県にしたので、実際に解析できる対象が45になりました。”

![imageJRL8-21-38](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-38.jpg)

D先生: “このように少ない情報量でも、RT法ではちゃんと異常検出できることに驚いたものです。確かに、東京と沖縄は極端なんですよね。逆に言うと、メンバ数が47もあるのであれば、SOART3メトリックスをつかって再評価したいものですが、チェビシェフ距離ではダメですか・・・。”

**（RT法の元祖：標準SN比の適用事例）**

![imageJRL8-21-39](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-39.jpg)

**（RT法分析で使った、経済系データ群）**

![imageJRL8-21-40](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-40.jpg)

QEU:FOUNDER: “RT法の元祖である標準SN比の事例でわかるように、**RT法は本来の姿としては項目間に「適切な関連性」がなければならない**。最低でも、全ての項目が「同一単位系」を持たなければなりません。一方、前述の都道府県の分析では、経済データの項目はバラバラでしょ？本来ならば、このような状態は「反則」です。”

D先生: “その反則を潜り抜けるために、あのときはMIN-MAXで規準化したんでしょ？”

**（再掲図11:ミンコフスキー距離の定義）**

![imageJRL8-21-11](/2024-02-05-QEUR23_VTRANS25/imageJRL8-21-11.jpg)

QEU:FOUNDER: “まあね・・・。ここで問題にしたいのは、画像判別以外では第3のメトリックスとしてチェビシェフ距離を使うのは避けたほうがいいということです。チェビシェフ距離は、特に極端な項目を選択する方法なんだけど、上記の「経済系データ」のチェビシェフ距離って意味があると思う？”

D先生: “確かに、画像判別以外ではチェビシェフ距離は意味がないです。”

QEU:FOUNDER: “だったら、**P=5あたりにしてミンコフスキー距離を使うのが面白い**と思います。実際には、pの値をいろいろ変えて比較して決めるべきですね。”

D先生: “何はともあれ、**RT法って出力するメトリックスがたった2次元しかないのが最大の長所であり、かつ弱点でした**。その意味では、RT法を3次元に拡張するのは、いままであまり使われなかったRT法を使うためには有効な手になるかもしれないですね。FOUNDERはp値の研究をやってみたいですか？”

### [＞寄付のお願い(click here)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

QEU:FOUNDER: “まあ、あとで時間があればやりましょう。あと、やる気が出ればね・・・(笑)。ともあれ、次は(SOART3法の)シングル・モードの開発です。”

