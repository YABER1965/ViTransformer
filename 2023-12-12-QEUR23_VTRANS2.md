---
title: QEUR23_VTRANS2: [Keras]花の画像の学習(CHANNEL=3)
date: 2023-12-12
tags: ["QEUシステム", "メトリックス", "Python言語", "Vision Transformer", "LLM", "データセット", "Fine-tuning", "イノベーション"]
excerpt: Vision Transformer(ViT)をやってみる
---

## QEUR23_VTRANS2: [Keras]花の画像の学習(CHANNEL=3)

## ～ まず、これは「たたき台」として・・・ ～

QEU:FOUNDER ： “じゃあ、今回はTensorflow(Keras)を使った、非常に簡単なViTの学習器をつくりましょう。これは、以下のYoutube動画からの引用なんです。”

[![MOVIE1](http://img.youtube.com/vi/Ssndsjh1Zqk/0.jpg)](http://www.youtube.com/watch?v=Ssndsjh1Zqk "Image Classification using Vision Transformer (ViT) in TensorFlow")

D先生 ： “確かに、このコメント欄（↓）からPYTHONのコードとデータセットが手に入るようですね。”

![imageJRL8-3-1](/2023-12-12-QEUR23_VTRANS2/imageJRL8-3-1.jpg)

QEU:FOUNDER ： “花のデータセットはそのまま使いました。あと、Pythonコードは、小生が少しだけ工夫をしています。それではプログラムをドン・・・。”

```python
# ------
# Flower classification(ORIGINAL VERSION)
# ------
!pip install opencv-python 
!pip install scikit-learn 
!pip install patchify

#####################

# ------
# VIT
import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model

# train-test
import numpy as np
import cv2
from glob import glob
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from patchify import patchify
from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStop-ping
# ------
import matplotlib.pyplot as plt

# ------
# CLASS - FUNCTION
# ------
# VIT
class ClassToken(Layer):
    def __init__(self):
        super().__init__()

    def build(self, input_shape):
        w_init = tf.random_normal_initializer()
        self.w = tf.Variable(
            initial_value = w_init(shape=(1, 1, input_shape[-1]), dtype=tf.float32),
            trainable = True
        )

    def call(self, inputs):
        batch_size = tf.shape(inputs)[0]
        hidden_dim = self.w.shape[-1]
        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])
        cls = tf.cast(cls, dtype=inputs.dtype)
        return cls

def mlp(x, cf):
    x = Dense(cf["mlp_dim"], activation="gelu")(x)
    x = Dropout(cf["dropout_rate"])(x)
    x = Dense(cf["hidden_dim"])(x)
    x = Dropout(cf["dropout_rate"])(x)
    return x

def transformer_encoder(x, cf):
    skip_1 = x
    x = LayerNormalization()(x)
    x = MultiHeadAttention(
        num_heads=cf["num_heads"], key_dim=cf["hidden_dim"]
    )(x, x)
    x = Add()([x, skip_1])

    skip_2 = x
    x = LayerNormalization()(x)
    x = mlp(x, cf)
    x = Add()([x, skip_2])

    return x

def ViT(cf):
    """ Inputs """
    input_shape = (cf["num_patches"], cf["patch_size"]*cf["patch_size"]*cf["num_channels"])
    inputs = Input(input_shape)     ## (None, 256, 3072)

    """ Patch + Position Embeddings """
    patch_embed = Dense(cf["hidden_dim"])(inputs)   ## (None, 256, 768)

    positions = tf.range(start=0, limit=cf["num_patches"], delta=1)
    pos_embed = Embedding(input_dim=cf["num_patches"], output_dim=cf["hidden_dim"])(positions) ## (256, 768)
    embed = patch_embed + pos_embed ## (None, 256, 768)

    """ Adding Class Token """
    token = ClassToken()(embed)
    x = Concatenate(axis=1)([token, embed]) ## (None, 257, 768)

    for _ in range(cf["num_layers"]):
        x = transformer_encoder(x, cf)

    """ Classification Head """
    x = LayerNormalization()(x)     ## (None, 257, 768)
    x = x[:, 0, :]
    x = Dense(cf["num_classes"], activation="softmax")(x)

    model = Model(inputs, x)
    return model

# -----
def create_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

def load_data(path, split=0.1):
    images = shuffle(glob(os.path.join(path, "*", "*.jpg")))

    split_size = int(len(images) * split)
    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)
    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)

    return train_x, valid_x, test_x

def process_image_label(path):
    """ Reading images """
    path = path.decode()
    image = cv2.imread(path, cv2.IMREAD_COLOR)
    image = cv2.resize(image, (hp["image_size"], hp["image_size"]))
    image = image/255.0

    """ Preprocessing to patches """
    patch_shape = (hp["patch_size"], hp["patch_size"], hp["num_channels"])
    patches = patchify(image, patch_shape, hp["patch_size"])

    patches = np.reshape(patches, hp["flat_patches_shape"])
    patches = patches.astype(np.float32)

    """ Label """
    class_name = path.split("/")[-2]
    class_idx = hp["class_names"].index(class_name)
    class_idx = np.array(class_idx, dtype=np.int32)

    return patches, class_idx

def parse(path):
    patches, labels = tf.numpy_function(process_image_label, [path], [tf.float32, tf.int32])
    labels = tf.one_hot(labels, hp["num_classes"])

    patches.set_shape(hp["flat_patches_shape"])
    labels.set_shape(hp["num_classes"])

    return patches, labels

def tf_dataset(images, batch=32):
    ds = tf.data.Dataset.from_tensor_slices((images))
    ds = ds.map(parse).batch(batch).prefetch(8)
    return ds

# -----
# Train-Test共通
""" Hyperparameters """
hp = {}
hp["image_size"] = 200
hp["num_channels"] = 3
hp["patch_size"] = 25
hp["num_patches"] = (hp["image_size"]**2) // (hp["patch_size"]**2)
hp["flat_patches_shape"] = (hp["num_patches"], hp["patch_size"]*hp["patch_size"]*hp["num_channels"])

hp["batch_size"] = 16
hp["lr"] = 1e-4
hp["num_epochs"] = 20    # 500
hp["num_classes"] = 5
hp["class_names"] = ["daisy", "dandelion", "roses", "sunflowers", "tulips"]

hp["num_layers"] = 12
hp["hidden_dim"] = 768
hp["mlp_dim"] = 3072
hp["num_heads"] = 12
hp["dropout_rate"] = 0.1

""" Seeding """
np.random.seed(42)
tf.random.set_seed(42)

""" Directory for storing files """
create_dir("files")

""" Paths """
dataset_path = "./flower_photos/"
model_path = os.path.join("files", "model.h5")
csv_path = os.path.join("files", "log.csv")

""" Dataset """
train_x, valid_x, test_x = load_data(dataset_path)
print(f"Train: {len(train_x)} - Valid: {len(valid_x)} - Test: {len(test_x)}")

train_ds = tf_dataset(train_x, batch=hp["batch_size"])
valid_ds = tf_dataset(valid_x, batch=hp["batch_size"])
test_ds = tf_dataset(test_x, batch=hp["batch_size"])

```

QEU:FOUNDER ： “このコードでは、花の画像のデータセットを訓練用、検証用、テスト用に分割しています。そして、分割されたデータセットは、それぞれTensorflowの機械学習用のテンソル型式に変換されています。”

![imageJRL8-3-2](/2023-12-12-QEUR23_VTRANS2/imageJRL8-3-2.jpg)

D先生 ： “このデータって画像(jpeg)なんでしょ？結構な数量がありますね。”

```python
def process_image_label(path):
    """ Reading images """
    path = path.decode()
    image = cv2.imread(path, cv2.IMREAD_COLOR)
    image = cv2.resize(image, (hp["image_size"], hp["image_size"]))
    image = image/255.0

```

QEU:FOUNDER ： “D先生、ViTの精度を上げるには多量の学習データがいるんでしょ？これで十分かどうかは、わからないね。あと、種々の大きさの画像があるので、**「リサイズ(resize)」**がなされています。”

### hp["image_size"] = 200

D先生 ： “画像がひずんじゃいますね。まあ、それはしょうがないんですが、肝心の花の画像の情報が大きく圧縮されていますね。”

```python
# -----
""" Train Model """
model = ViT(hp)
model.compile(
    loss="categorical_crossentropy",
    optimizer=tf.keras.optimizers.Adam(hp["lr"], clipvalue=1.0),
    metrics=["acc"]
)

callbacks = [
    ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-10, verbose=1),
    CSVLogger(csv_path),
    EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=False),
]

history = model.fit(
    train_ds,
    epochs=hp["num_epochs"],
    validation_data=valid_ds,
    callbacks=callbacks
)
```

QEU:FOUNDER ： “今回はKerasを使用しているので、学習状況の出力が簡単なのが助かるよね。”

![imageJRL8-3-3](/2023-12-12-QEUR23_VTRANS2/imageJRL8-3-3.jpg)

D先生 ： “やっぱり計算に時間がかかりますか？GPUを使っているんでしょ？”

QEU:FOUNDER ： “1エポックあたり3分ぐらいかな？GPUはT4相当を使っています。実際に現場で使うときも、T4相当のGPUに抑えたいよね。”

D先生 ： “一方、**LLM（自然言語処理）のモデルのほとんどがA100クラスのGPUが必要**なのはキツイですよね。そうだ！だから、FOUNDERはLLMの学習を後回しにしたんでしょう？”

QEU:FOUNDER ： “バレたか・・・（笑）。”

```python
# ------
# 学習履歴曲線を描く
# ------
def plot_history(item):
    plt.plot(history.history[item], label=item)
    plt.plot(history.history["val_" + item], label="val_" + item)
    plt.xlabel("Epochs")
    plt.ylabel(item)
    plt.title("Train and Validation {} Over Epochs".format(item), fontsize=14)
    plt.legend()
    plt.grid()
    plt.show()

plot_history("loss")
plot_history("acc")
```

QEU:FOUNDER ： “今回は20エポックだけ学習しました。まあ、100回もいらないよね、トライアル段階なんだから・・・。”

**(Lossの推移)**

![imageJRL8-3-4](/2023-12-12-QEUR23_VTRANS2/imageJRL8-3-4.jpg)

D先生 ： “確かに、15エポック程度で精度が飽和していますよね。このモデルの正確度は、いいところ60％ぐらいか・・・。”

**(Accuracyの推移)**

![imageJRL8-3-5](/2023-12-12-QEUR23_VTRANS2/imageJRL8-3-5.jpg)

QEU:FOUNDER ： “精度の詳細の議論は、後でやりましょう。つづきをいきます。”

```python
"""
Let's display patches for a sample image
"""
from PIL import Image 

plt.figure(figsize=(4, 4))
image_name = test_x[0]
img = (Image.open(image_name)) 
# We can display the image 
print("--- original_image---")
print(image_name)
#display(img)
# ---
image = np.asarray(img) 
plt.imshow(image.astype("uint8"))
plt.axis("off")
```

QEU:FOUNDER ： “画像を読み込んで、予測をしてみましょう。ここで、再度注意をしてほしいのは、モデルへのインプットはtensorFlowのデータセット変数であり、ユーザー定義された関数**「tf_dataset」**で処理されています。”

![imageJRL8-3-6](/2023-12-12-QEUR23_VTRANS2/imageJRL8-3-6.jpg)

```python
# -----
# PREDICTION OF SAMPLE IMAGE
pred_x  = [test_x[0]]
print(pred_x)
# ---
pred_ds = tf_dataset(pred_x, batch=hp["batch_size"])
arr_y = model.predict(pred_ds)
print("--- prediction_image---")
print(arr_y)
# ---
items_class = hp["class_names"] 
#print(items_class)
index_max = arr_y[0].argmax()
print(items_class[index_max])
```

D先生 ： “今回の予測は、うまい具合に正解（バラ）でしたね。人間が見ても、明らかにバラの画像なので、機械でもわかりやすかったのでしょう。”

![imageJRL8-3-7](/2023-12-12-QEUR23_VTRANS2/imageJRL8-3-7.jpg)

D先生 ： “そして、最後はテスト用のデータセットで確認しています。正確度が60％弱程度じゃ、まだまだですよね。やっぱり、データ数の問題？”

```python
# -----
""" Evaluate Model """
model.evaluate(test_ds)

```

![imageJRL8-3-8](/2023-12-12-QEUR23_VTRANS2/imageJRL8-3-8.jpg)

QEU:FOUNDER ： “そんなに短絡的に決めつけないで、まずはデータセットの内容を見たほうがいいんじゃない？”

![imageJRL8-3-9](/2023-12-12-QEUR23_VTRANS2/imageJRL8-3-9.jpg)

D先生 ： “これは「roses」データセット内のメンバか・・・。あれあれ！？”

![imageJRL8-3-10](/2023-12-12-QEUR23_VTRANS2/imageJRL8-3-10.jpg)

C部長 ： “なんか、ポツポツと変なものが混じっています。これを学習しているようじゃあ、予測精度が上がらないですよ！！”

QEU:FOUNDER ： “今回の開発で、手法毎の実力差を知りたいのであれば、最初に**「それなりの質をもつデータセット」**を揃えたほうがいいよ。あと、学習データ量を増やすんだったら、画像を回転させたり、反転させたりしても良いと思います。”

D先生 ： “わざと反転画像や回転画像を学習させると、モデルが特徴を効率的に学習するらしいですね。”

QEU:FOUNDER ： “我々の目標は検査のためにViTを使うことでしょ？このようなバラバラの画像はありえないって・・・。今後は、いろいろ手法を変えてコードを発展させたいが、第一歩としては「評価のためのベース固め」に行きましょう。”


## ～ まとめ ～

C部長 : “えっと・・・、今回のテーマは**「創造力」**ですが、同じような話は昔もやりましたね。”

[![MOVIE2](http://img.youtube.com/vi/twEP9eHgP9c/0.jpg)](http://www.youtube.com/watch?v=twEP9eHgP9c "【グーグル日本法人元社長辻野氏と語る「AIのリスク」と「“安倍政治の負の遺産”のリスク」】郷原信郎の「日本の権力を斬る！」")

QEU:FOUNDER ： “重要なテーマであるならば、もう一度話すことは重要な意味があります。”

![imageJRL8-3-11](/2023-12-12-QEUR23_VTRANS2/imageJRL8-3-11.jpg)

QEU:FOUNDER ： “G社がこのような「個人を開放する」というやり方を取るのは、そうしないとG社に仕事がなくなるからなんだよね。彼らがシステムを開発すると、**「その問題を根本的に解決する」**から・・・。つまり、一旦最先端のシステムを採用すると、人手で何とかした**「古き良き昭和（↓）には永遠に戻れなくなる」**んです。わざとやりたくないのであれば、それは別だがね・・・。”

### オッサン（＠車中、N社検査不正について）： 「“検査不正”っていうのはなァ、（組織外に不正を）漏らしたヤツが悪いんだよ・・・」

C部長 : “それにしても、G社って、なぜあんなに太っ腹なんでしょうか？あんなにすごい技術をドンドン開発して、そのシステムを惜しげもなく、皆さんに開放しているじゃないですか・・・。”

QEU:FOUNDER ： “理由としては、オープン戦略で開発競争が激しくなって、その結果として開発が速くなるのがあるよね。あと、もっと重要なのは、**現在の技術で最も重要なのは「データセット」**なんですよ。彼らは、ロジックは公開しても、自分らがモデルの学習に使っているデータセットを公開していないですよ。”

C部長 : “逆に言うと、我々が今までLLMとかでデータセットを手作りでやっていたのは、実は大きな意味があったと・・・？”

![imageJRL8-3-12](/2023-12-12-QEUR23_VTRANS2/imageJRL8-3-12.jpg)

QEU:FOUNDER ： “実は、**一見単純に見えるデータセットの中にこそ独創性がある**んです！！それにしても「何をしなくても大いに結構」って・・・。闇雲に若者を批判する人たちは、今、自分たちが本当にやるべきことをわかっているんかなぁと思います。まあ、たぶん、とっても偉い人たちなので、イノベーションは彼らがなんとかしてくれると思いますよ。小生は能力がないので、ちまちまとデータセットを開発しますよ（笑)。
