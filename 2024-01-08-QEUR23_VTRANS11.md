---
title: QEUR23_VTRANS11: [PyTorch]ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
date: 2024-01-08
tags: ["QEUã‚·ã‚¹ãƒ†ãƒ ", "ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹", "Pythonè¨€èª", "Vision Transformer", "LLM", "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ", "Fine-tuning", "ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³"]
excerpt: Vision Transformer(ViT)ã‚’ã‚„ã£ã¦ã¿ã‚‹
---

## QEUR23_VTRANS11: [PyTorch]ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

## ï½ ä»Šå›ã¯ã‚ãã¾ã§ã€Œä¾‹ã€ã§ã™ã‹ã‚‰ã­ãƒ»ãƒ»ãƒ» ï½

### ãƒ»ãƒ»ãƒ» å‰å›ã®ã¤ã¥ãã§ã™ ãƒ»ãƒ»ãƒ»

QEU:FOUNDER ï¼š â€œã‚‚ã¡ã‚ã‚“ã€ã“ã®æœ€çµ‚ç›®çš„ã¯ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒãƒƒãƒ—ã®ç”Ÿæˆã§ã™ã€‚ã“ã‚Œã‚‚ã€è¨ˆç®—ã—ãŸçµæœã ã‘ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚ã“ã®å‡ºæ¥ã§ã‚ã‚Œã°ã€ç¾å ´ã§ã‚‚ä½¿ãˆã‚‹ã§ã—ã‚‡ã†ï¼Ÿâ€

![imageJRL8-12-1](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-1.jpg)

Céƒ¨é•· : â€œã‚‚ã¡ã‚ã‚“ï¼ãƒ»ãƒ»ãƒ»ãã†ã‹ã€è£½å“ã®å†™çœŸï¼ˆè‰¯å“ã€ä¸è‰¯å“ï¼‰ã‚’ã€Œå¾¹åº•çš„ã«é›†ã‚ã‚‹ã€ã¨è‡ªå‹•æ¤œæŸ»æ©ŸãŒã§ãã‚‹ã®ã‹ãƒ»ãƒ»ãƒ»ã€‚ãã†ã„ãˆã°ã€**ã€Œã‚ã¨ã§ãƒ¬ã‚¤ãƒ¤ãƒ¼æ¯ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—ã‚’æã„ã¦ãã‚Œã‚‹ã€**ã£ã¦ã€è¨€ã£ã¦ã„ãŸã˜ã‚ƒãªã„ã§ã™ã‹ï¼Ÿâ€

QEU:FOUNDER ï¼š â€œã˜ã‚ƒã‚ã€Céƒ¨é•·æ§˜ã®ã”æœŸå¾…ã«å¿œãˆã¦ViTãƒ¬ã‚¤ãƒ¤ãƒ¼æ¯ã®ãƒãƒƒãƒ—ã®ä½œè£½ã«ã„ãã¾ã—ã‚‡ã†ã€‚ãã‚Œã¯ã€ç°¡å˜ãªãƒ—ãƒ­ã‚°ãƒ©ãƒ æ”¹é€ ã§ã§ãã¾ã™ã€‚â€

```python
##############################
# ATTENTON MAP(INCLUDING LOAD MODEL)
##############################
#@title ViT Implementation ğŸ”¥
import math
import torch
from torch import nn

ã°ã£ã•ã‚Šå‰Šé™¤

# ------
# Create the model, optimizer, loss function and trainer
model = ViTForClassfication(config)
#print(model)

# ----
# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹
BATCH_SIZE = 32  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆæ™‚ã¨åŒã˜

# -------
from torchinfo import summary

# display model's structure
summary(model=model,
        input_size=(BATCH_SIZE, 3, 224, 224), # (batch_size, num_patches, embedding_dimension)
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"])
		
# ------
classes = ('angular_leaf_spot', 'bean_rust', 'healthy')
# ------
# Load model
experiment_name = 'vit-bean-20-epochs' #@param {type:"string"}
# ------
config, model, train_losses, test_losses, accuracies = load_experiment(experiment_name, model)

# ------
# Create Dataset and DataLoader
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader

# ------
test_transform = transforms.Compose(
    [transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

test_dataset = ImageFolder(root="bean_leaf_TEST", transform=test_transform)
testloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)

# -------
# çµæœã‚’äºˆæ¸¬ã™ã‚‹(ãƒãƒƒãƒåˆ†ã ã‘å‡ºåŠ›ã™ã‚‹)
model.eval()
with torch.no_grad():
    icount = 0
    for batch in testloader:
        if icount == 0:
            # Move the batch to the device
            batch = [t.to(device) for t in batch]
            images, labels = batch
            #print(images.shape)
            # Get predictions
            model = model.to(device)
            logits, attention_maps = model(images, output_attentions=True)
            icount = icount + 1
# ----
# ã‚¤ãƒ¡ãƒ¼ã‚¸ã¨ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆã™ã‚‹
np_images = images.cpu().detach().numpy()
np_images = np.transpose(np_images, (0,2,3,1))
np_labels = labels.cpu().detach().numpy()
# ----
# äºˆæ¸¬ã¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒãƒƒãƒ—ã‚’ç”Ÿæˆã™ã‚‹
predictions = torch.argmax(logits, dim=1)
np_predictions = predictions.cpu().detach().numpy()
# Concatenate the attention maps from all blocks
attention_maps = torch.cat(attention_maps, dim=1)
# select only the attention maps of the CLS token
attention_maps = attention_maps[:, :, 0, 1:]
np_attention_maps = attention_maps.cpu().detach().numpy()

# ------
def visualize_attention_layer(i_layer, num_images, images, labels, predictions, attention_maps, out-put):
    """
    Visualize the attention maps of the first 9 images.
    """
    classes = ('angular_leaf_spot', 'bean_rust', 'healthy')
    # Pick images
    raw_images = [np.asarray(images[i]) for i in range(num_images)]
    labels = [labels[i] for i in range(num_images)]
    # ---
    # Then average the attention maps of the CLS token over all the heads
    #print(attention_maps.shape)
    # torch.Size([32, 16, 196])
    #attention_maps = attention_maps.mean(dim=1)
    attention_maps = attention_maps[:,i_layer,:]
    #print(attention_maps.shape)
    # torch.Size([32, 196])
    # Reshape the attention maps to a square
    num_patches = attention_maps.size(-1)
    size = int(math.sqrt(num_patches))
    attention_maps = attention_maps.view(-1, size, size)
    # Resize the map to the size of the image
    attention_maps = attention_maps.unsqueeze(1)
    attention_maps = F.interpolate(attention_maps, size=(224, 224), mode='bilinear', align_corners=False)
    attention_maps = attention_maps.squeeze(1)
    # Plot the images and the attention maps
    fig = plt.figure(figsize=(16, 12))
    mask = np.concatenate([np.ones((224, 224)), np.zeros((224, 224))], axis=1)
    for i in range(num_images):
        ax = fig.add_subplot(3, 3, i+1, xticks=[], yticks=[])
        img = np.concatenate((raw_images[i], raw_images[i]), axis=1)
        ax.imshow(img)
        # Mask out the attention map of the left image
        extended_attention_map = np.concatenate((np.zeros((224, 224)), attention_maps[i].cpu()), ax-is=1)
        extended_attention_map = np.ma.masked_where(mask==1, extended_attention_map)
        ax.imshow(extended_attention_map, alpha=0.5, cmap='jet')
        # Show the ground truth and the prediction
        gt = classes[labels[i]]
        pred = classes[predictions[i]]
        ax.set_title(f"gt: {gt} / pred: {pred}", color=("green" if gt==pred else "red"))
    plt.tight_layout()
    if output is not None:
        plt.savefig(output)
    plt.show()

# ------
num_images = 9

# ------
#@title Visualize Attetion
for i_layer in range(12): 
    print("LayerNo :",i_layer)
    visualize_attention_layer(i_layer, num_images, np_images, np_labels, np_predictions, atten-tion_maps, "attention_bean_layer{}.png".format(i_layer))

```

Då…ˆç”Ÿ ï¼š â€œã¾ãšã¯ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼0ã‹ã‚‰è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ â€œ

**ï¼ˆãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼š0ï¼‰**

![imageJRL8-12-2](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-2.jpg)

Då…ˆç”Ÿ ï¼š â€œãƒ¬ã‚¤ãƒ¤ãƒ¼0ã®ãƒãƒƒãƒ—ã¯ã€è‘‰ã®å…¨ä½“ã‚’è¦‹ã¦ã„ã‚‹ã‚ˆã†ãªå°è±¡ãŒã‚ã‚Šã¾ã™ã€‚â€œ

QEU:FOUNDER ï¼š â€œã˜ã‚ƒã‚ã€æ¬¡ã¯ãƒ¬ã‚¤ãƒ¤ãƒ¼1ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚â€


**ï¼ˆãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼š1ï¼‰**

![imageJRL8-12-3](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-3.jpg)

Céƒ¨é•· : â€œãŠã£ã¨ãƒ»ãƒ»ãƒ»ã€‚ã“ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯ç•°å¸¸éƒ¨ã‚’è¦‹ã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚â€

QEU:FOUNDER ï¼š â€œã˜ã‚ƒã‚ã€æ®‹ã‚Š3ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ã¿ã¦ã¿ã¾ã—ã‚‡ã†ã€‚â€

**ï¼ˆãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼š2ï¼‰**

![imageJRL8-12-4](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-4.jpg)

**ï¼ˆãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼š3ï¼‰**

![imageJRL8-12-5](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-5.jpg)

**ï¼ˆãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼š4ï¼‰**

![imageJRL8-12-6](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-6.jpg)

Då…ˆç”Ÿ ï¼š â€œç¢ºã‹ã«ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼æ¯ã«è¦‹ã¦ã„ã‚‹ãƒã‚¤ãƒ³ãƒˆãŒå°‘ã—ã¥ã¤å¤‰ã‚ã£ã¦ã„ã¾ã™ã€‚ã˜ã‚ƒã‚ã€FOUNDERã€ã²ã¨ã¤ææ¡ˆãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã†ã¡ã€**ã€Œï¼ˆç•°å¸¸æ¤œå‡ºã«ã¨ã£ã¦ï¼‰è‰¯ã•ã’ãªãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿ã‚’ä½¿ã£ã¦ã€å¹³å‡ãƒãƒƒãƒ—ã‚’ã¤ãã‚‹**ã“ã¨ã¯ã§ããªã„ã§ã—ã‚‡ã†ã‹ï¼Ÿâ€œ

### attention_maps = attention_maps[:,[1, 3, 4, 6, 7, 8, 10, 11],:].mean(dim=1)

QEU:FOUNDER ï¼š â€œä»¥ä¸‹ã®ã‚ˆã†ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’å¤‰æ›´ã™ã‚Œã°ç°¡å˜ã«ã¤ãã‚Œã¾ã™ã‚ˆã€‚è¨ˆç®—çµæœã‚’ãƒ‰ãƒ³ãƒ»ãƒ»ãƒ»ã€‚â€

![imageJRL8-12-7](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-7.jpg)

QEU:FOUNDER ï¼š â€œã©ã†ï¼Ÿå‰ã‚ˆã‚Šã‚‚è‰¯ããªã£ãŸï¼Ÿâ€

Då…ˆç”Ÿ ï¼š â€œã„ã‚„ã‚ã€ãªã‚“ã¨ã‚‚ãƒ»ãƒ»ãƒ»ï¼ˆç¬‘ï¼‰ã€‚ã—ã‹ã—ã€ã“ã®ã‚ˆã†ã«ã€Œãƒã‚¤ã‚ºã€ã‚’æ¶ˆã™ã¨ã„ã†æ‰‹æ®µã¯ãƒãƒƒãƒ—ã®å‡ºæ¥ã‚’æ”¹å–„ã™ã‚‹æ–¹æ³•ã¨ã—ã¦ã¯ã€Œã‚ã‚‹ã€ã¨ã¯æ€ã„ã¾ã™ã€‚ç¾åœ¨ã¯å˜ã«ãƒ†ã‚¹ãƒˆæ®µéšã§ã™ã‹ã‚‰ã€ã¾ã ã¾ã å­¦ç¿’ãŒãŸã‚Šã¾ã›ã‚“ã€‚ç§ã‚‚(è‰¯ã—æ‚ªã—ã®æœ€çµ‚)åˆ¤æ–­ã¯æ§ãˆã¾ã™ã€‚â€

Céƒ¨é•· : â€œã“ã“ã¾ã§ã¯ã€ViTã‚’çŸ¥ã£ã¦ã„ã‚‹äººã ã£ãŸã‚‰èª°ã§ã‚‚ã§ãã‚‹ä½œæ¥­ã§ã™ã€‚ã“ã‚Œã‹ã‚‰ãŒã€æˆ‘ã€…ã®ã‚ªãƒªã‚¸ãƒŠãƒªãƒ†ã‚£ã®è¦‹ã›æ‰€ã§ã™ã‚ˆã­ã€‚ã­ãˆï¼FOUNDERãƒ»ãƒ»ãƒ»ã€‚â€

### [ï¼å¯„ä»˜ã®ãŠé¡˜ã„(click here)ï¼œ](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

QEU:FOUNDER  ï¼š â€œå¦™ãªãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã‚’ã‹ã‘ãªã„ã§ã„ãŸã ããŸã„ã€‚â€


## ï½ ã¾ã¨ã‚ ï½

### ãƒ»ãƒ»ãƒ» å‰å›ã®ã¤ã¥ãã§ã™ ãƒ»ãƒ»ãƒ»

QEU:FOUNDER ï¼š â€œï¼ˆæˆ‘ã€…ã®ï¼‰æœ€çµ‚ç›®æ¨™ã¯ã€ã ã‚Œã‚‚ãŒå¤–è¦³æ¤œæŸ»ã«ç°¡å˜ã«ã¤ã‹ãˆã‚‹ã€Œãƒ—ãƒ¬ãƒ»ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆãƒ¢ãƒ‡ãƒ«ã€ã‚’ç¢ºç«‹ã™ã‚‹ã“ã¨ã§ã™ã€‚ä»Šå›ã®ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã§ã€æ—¢å­˜ã®åˆ†é¡ãƒ¢ãƒ‡ãƒ«ãŒæ¤œæŸ»ï¼ˆç•°å¸¸æ¤œå‡ºï¼‰ã«ä½¿ãˆãªã„ã“ã¨ãŒã‚ã‹ã£ãŸã§ã—ã‚‡ï¼Ÿâ€

Då…ˆç”Ÿ ï¼š â€œã“ã‚ŒãŒJå›½å¾©æ´»ã®æœ€ä½æ¡ä»¶ã‹ãƒ»ãƒ»ãƒ»ã€‚ãªã«ã¯ã¨ã‚‚ã‚ã‚Œã€å…·ä½“çš„ã§ã™ã­ã€‚è¦æ±‚äº‹é …ãŒãƒ»ãƒ»ãƒ»ã€‚â€

![imageJRL8-12-8](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-8.jpg)

QEU:FOUNDER ï¼š â€œã“ã®ã¾ã¾å´©å£Šã€æ²ˆã‚“ã§ã„ããŸã‘ã‚Œã°ã€ã¹ã¤ã«ã‚„ã‚‰ãªãã¦ã‚‚ã„ã„ã‚ˆã€‚ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¦‹ã¦ã€ã€Œãµï½ã‚“ã€ã¨ã—ã‹æ€ã‚ãªã„äººã‚‚ã„ã‚‹ã ã‚ã†ã—ãƒ»ãƒ»ãƒ»ã€‚â€

![imageJRL8-12-9](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-9.jpg)

Då…ˆç”Ÿ ï¼š â€œãã¬ã¬ãƒ»ãƒ»ãƒ»ã€‚ãƒ»ãƒ»ãƒ»ã¨æ€ã„ã¾ã—ãŸã€‚FOUNDERã¯ï¼Ÿâ€

QEU:FOUNDER ï¼š â€œãã‚Šã‚ƒã€ãã†ã ã‚ã†ã¨ãƒ»ãƒ»ãƒ»ã€‚ãŸã ã—ã€ã¨ã«ã‹ãã€Œçš†ãŒã€å¹¸ã›ã§ã‚ã‚Œã°ã„ã„ã‚“ã§ã—ã‚‡ã†ã­ã€‚â€

Då…ˆç”Ÿ ï¼š â€œ**ã€Œçš†ãŒå¹¸ã›ã€**ãªã‚“ã§ã™ã‹ï¼Ÿã„ã¾ã¯ï¼Ÿâ€

![imageJRL8-12-10](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-10.jpg)

QEU:FOUNDER ï¼š â€œã•ã‚ã­ãƒ»ãƒ»ãƒ»ã€‚ãŸã ã—ã€21ä¸–ç´€ã®åˆé ­ã«ã€å…¨ã¦ã®ä¼šç¤¾ãŒé–‹ç™ºå·¥æœŸã®çŸ­ç¸®ã€è£½é€ ã‚³ã‚¹ãƒˆã‚’å®‰ãå®‰ããƒ»ãƒ»ãƒ»ã€‚ã•ã‚‰ã«ã¯ã€ï¼ˆä¼šç¤¾ãŒï¼‰ã‚¹ãƒªãƒ ã«ãªã£ã¦ãƒ»ãƒ»ãƒ»ã€‚ãƒ»ãƒ»ãƒ»ãªã‚“ã¨ã‹ã§ç¤¾ä¼šå…¨ä½“ãŒ**ã€Œãƒ‰ãƒ„ãƒœã«ã¯ã¾ã£ãŸã€**
ã“ã¨ã‚’æ€ã„å‡ºã™ã¨ããŒæ¥ã¾ã—ãŸã€‚**ã€Œã‚µãƒ—ãƒ©ã‚¤ãƒ¤ã¯è‡ªç¤¾ã®è£½å“ã®ãŠå®¢æ§˜ã€ã¯é‡è¦ãªç¤ºå”†**ã ã¨æ€ã„ã¾ã™ã€‚â€

