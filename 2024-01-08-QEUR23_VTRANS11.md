---
title: QEUR23_VTRANS11: [PyTorch]アテンションマップのチューニング
date: 2024-01-08
tags: ["QEUシステム", "メトリックス", "Python言語", "Vision Transformer", "LLM", "データセット", "Fine-tuning", "イノベーション"]
excerpt: Vision Transformer(ViT)をやってみる
---

## QEUR23_VTRANS11: [PyTorch]アテンションマップのチューニング

## ～ 今回はあくまで「例」ですからね・・・ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “もちろん、この最終目的はアテンション・マップの生成です。これも、計算した結果だけを出力します。この出来であれば、現場でも使えるでしょう？”

![imageJRL8-12-1](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-1.jpg)

C部長 : “もちろん！・・・そうか、製品の写真（良品、不良品）を「徹底的に集める」と自動検査機ができるのか・・・。そういえば、**「あとでレイヤー毎のアテンションマップを描いてくれる」**って、言っていたじゃないですか？”

QEU:FOUNDER ： “じゃあ、C部長様のご期待に応えてViTレイヤー毎のマップの作製にいきましょう。それは、簡単なプログラム改造でできます。”

```python
##############################
# ATTENTON MAP(INCLUDING LOAD MODEL)
##############################
#@title ViT Implementation 🔥
import math
import torch
from torch import nn

ばっさり削除

# ------
# Create the model, optimizer, loss function and trainer
model = ViTForClassfication(config)
#print(model)

# ----
# データローダーを生成する
BATCH_SIZE = 32  # バッチサイズはモデル生成時と同じ

# -------
from torchinfo import summary

# display model's structure
summary(model=model,
        input_size=(BATCH_SIZE, 3, 224, 224), # (batch_size, num_patches, embedding_dimension)
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"])
		
# ------
classes = ('angular_leaf_spot', 'bean_rust', 'healthy')
# ------
# Load model
experiment_name = 'vit-bean-20-epochs' #@param {type:"string"}
# ------
config, model, train_losses, test_losses, accuracies = load_experiment(experiment_name, model)

# ------
# Create Dataset and DataLoader
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader

# ------
test_transform = transforms.Compose(
    [transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

test_dataset = ImageFolder(root="bean_leaf_TEST", transform=test_transform)
testloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)

# -------
# 結果を予測する(バッチ分だけ出力する)
model.eval()
with torch.no_grad():
    icount = 0
    for batch in testloader:
        if icount == 0:
            # Move the batch to the device
            batch = [t.to(device) for t in batch]
            images, labels = batch
            #print(images.shape)
            # Get predictions
            model = model.to(device)
            logits, attention_maps = model(images, output_attentions=True)
            icount = icount + 1
# ----
# イメージとラベルを生成する
np_images = images.cpu().detach().numpy()
np_images = np.transpose(np_images, (0,2,3,1))
np_labels = labels.cpu().detach().numpy()
# ----
# 予測とアテンション・マップを生成する
predictions = torch.argmax(logits, dim=1)
np_predictions = predictions.cpu().detach().numpy()
# Concatenate the attention maps from all blocks
attention_maps = torch.cat(attention_maps, dim=1)
# select only the attention maps of the CLS token
attention_maps = attention_maps[:, :, 0, 1:]
np_attention_maps = attention_maps.cpu().detach().numpy()

# ------
def visualize_attention_layer(i_layer, num_images, images, labels, predictions, attention_maps, out-put):
    """
    Visualize the attention maps of the first 9 images.
    """
    classes = ('angular_leaf_spot', 'bean_rust', 'healthy')
    # Pick images
    raw_images = [np.asarray(images[i]) for i in range(num_images)]
    labels = [labels[i] for i in range(num_images)]
    # ---
    # Then average the attention maps of the CLS token over all the heads
    #print(attention_maps.shape)
    # torch.Size([32, 16, 196])
    #attention_maps = attention_maps.mean(dim=1)
    attention_maps = attention_maps[:,i_layer,:]
    #print(attention_maps.shape)
    # torch.Size([32, 196])
    # Reshape the attention maps to a square
    num_patches = attention_maps.size(-1)
    size = int(math.sqrt(num_patches))
    attention_maps = attention_maps.view(-1, size, size)
    # Resize the map to the size of the image
    attention_maps = attention_maps.unsqueeze(1)
    attention_maps = F.interpolate(attention_maps, size=(224, 224), mode='bilinear', align_corners=False)
    attention_maps = attention_maps.squeeze(1)
    # Plot the images and the attention maps
    fig = plt.figure(figsize=(16, 12))
    mask = np.concatenate([np.ones((224, 224)), np.zeros((224, 224))], axis=1)
    for i in range(num_images):
        ax = fig.add_subplot(3, 3, i+1, xticks=[], yticks=[])
        img = np.concatenate((raw_images[i], raw_images[i]), axis=1)
        ax.imshow(img)
        # Mask out the attention map of the left image
        extended_attention_map = np.concatenate((np.zeros((224, 224)), attention_maps[i].cpu()), ax-is=1)
        extended_attention_map = np.ma.masked_where(mask==1, extended_attention_map)
        ax.imshow(extended_attention_map, alpha=0.5, cmap='jet')
        # Show the ground truth and the prediction
        gt = classes[labels[i]]
        pred = classes[predictions[i]]
        ax.set_title(f"gt: {gt} / pred: {pred}", color=("green" if gt==pred else "red"))
    plt.tight_layout()
    if output is not None:
        plt.savefig(output)
    plt.show()

# ------
num_images = 9

# ------
#@title Visualize Attetion
for i_layer in range(12): 
    print("LayerNo :",i_layer)
    visualize_attention_layer(i_layer, num_images, np_images, np_labels, np_predictions, atten-tion_maps, "attention_bean_layer{}.png".format(i_layer))

```

D先生 ： “まずは、レイヤー0から見てみましょう。 “

**（レイヤー：0）**

![imageJRL8-12-2](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-2.jpg)

D先生 ： “レイヤー0のマップは、葉の全体を見ているような印象があります。“

QEU:FOUNDER ： “じゃあ、次はレイヤー1を見てみましょう。”


**（レイヤー：1）**

![imageJRL8-12-3](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-3.jpg)

C部長 : “おっと・・・。このレイヤーは異常部を見ているようです。”

QEU:FOUNDER ： “じゃあ、残り3レイヤーをみてみましょう。”

**（レイヤー：2）**

![imageJRL8-12-4](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-4.jpg)

**（レイヤー：3）**

![imageJRL8-12-5](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-5.jpg)

**（レイヤー：4）**

![imageJRL8-12-6](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-6.jpg)

D先生 ： “確かに、レイヤー毎に見ているポイントが少しづつ変わっています。じゃあ、FOUNDER、ひとつ提案があります。これらのレイヤーのうち、**「（異常検出にとって）良さげなレイヤーのみを使って」平均マップをつくる**ことはできないでしょうか？“

### attention_maps = attention_maps[:,[1, 3, 4, 6, 7, 8, 10, 11],:].mean(dim=1)

QEU:FOUNDER ： “以下のようにプログラムを変更すれば簡単につくれますよ。計算結果をドン・・・。”

![imageJRL8-12-7](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-7.jpg)

QEU:FOUNDER ： “どう？前よりも良くなった？”

D先生 ： “いやあ、なんとも・・・（笑）。しかし、このように「ノイズ」を消すという手段はマップの出来を改善する方法としては「ある」とは思います。現在は単にテスト段階ですから、まだまだ学習がたりません。私も(良し悪しの最終)判断は控えます。”

C部長 : “ここまでは、ViTを知っている人だったら誰でもできる作業です。これからが、我々のオリジナリティの見せ所ですよね。ねえ！FOUNDER・・・。”

### [＞寄付のお願い(click here)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

QEU:FOUNDER  ： “妙なプレッシャーをかけないでいただきたい。”


## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “（我々の）最終目標は、だれもが外観検査に簡単につかえる「プレ・トレーニング済モデル」を確立することです。今回のトライアルで、既存の分類モデルが検査（異常検出）に使えないことがわかったでしょ？”

D先生 ： “これがJ国復活の最低条件か・・・。なにはともあれ、具体的ですね。要求事項が・・・。”

![imageJRL8-12-8](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-8.jpg)

QEU:FOUNDER ： “このまま崩壊、沈んでいきたければ、べつにやらなくてもいいよ。以下のニュースを見て、「ふ～ん」としか思わない人もいるだろうし・・・。”

![imageJRL8-12-9](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-9.jpg)

D先生 ： “ぐぬぬ・・・。・・・と思いました。FOUNDERは？”

QEU:FOUNDER ： “そりゃ、そうだろうと・・・。ただし、とにかく「皆が」幸せであればいいんでしょうね。”

D先生 ： “**「皆が幸せ」**なんですか？いまは？”

![imageJRL8-12-10](/2024-01-08-QEUR23_VTRANS11/imageJRL8-12-10.jpg)

QEU:FOUNDER ： “さあね・・・。ただし、21世紀の初頭に、全ての会社が開発工期の短縮、製造コストを安く安く・・・。さらには、（会社が）スリムになって・・・。・・・なんとかで社会全体が**「ドツボにはまった」**
ことを思い出すときが来ました。**「サプライヤは自社の製品のお客様」は重要な示唆**だと思います。”

