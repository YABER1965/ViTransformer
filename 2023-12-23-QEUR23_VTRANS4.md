---
title: QEUR23_VTRANS4: [Keras]ファインチューニングはやっぱりすごい！！
date: 2023-12-23
tags: ["QEUシステム", "メトリックス", "Python言語", "Vision Transformer", "LLM", "データセット", "Fine-tuning", "イノベーション"]
excerpt: Vision Transformer(ViT)をやってみる
---

## QEUR23_VTRANS4: [Keras]ファインチューニングはやっぱりすごい！！

## ～ たった、これだけのデータで・・・ ～

C部長 : “ちょっと、前回のプロジェクトから時間が空きましたね・・・。なんか、「ガタガタ」やってたんですか？”

**（TRAIN用データセット）**

![imageJRL8-5-1](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-1.jpg)

QEU:FOUNDER ： “学習させるデータの構造を変えたんですよ。具体的には、いままでは1つのデータセット（TRAIN）を訓練・検証・評価用にシェアしたんですが、そこの考え方を変えました。評価（TEST）は別データセットにしました。”

C部長 : “確かに、現在のデータ数（9000件程度）では少ないです。ちょっとだけ増やしたというだけですか？”

**（TRAIN用データセット）**

![imageJRL8-5-2](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-2.jpg)

QEU:FOUNDER ： “そういえば、そういう(データ数up)メリットもあったっけ・・・（笑）。	小生の目的は、TRAIN-TESTの間でデータセットの質を変えたんです。ココに挙げた、TRAINデータセット（↑）とTESTデータセット（↓）を比較してください。両者の質がかなり違うでしょ？”

**（TEST用：その１）**

![imageJRL8-5-3](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-3.jpg)

D先生 ： “テスト用には、猫ちゃんとか、絵とかがずいぶん増えてきていますね。一方、TRAIN用では普通の花の写真が多いです。”

QEU:FOUNDER  ： “ＡＩって、コンピュータに人間と同じ認識能力を持たせるプロジェクトの総称です。そういう意味で、あくまで人間をベースに考えなければなりません。そう考えてみると、「人間って不思議だなぁ」と思わない？我々（人間）は、TEST画像のラフな画像を見ても、これはヒマワリだ、タンポポだとわかるわけ・・・。”

D先生 ： “なぜなんでしょうね？”

**（TEST用：その２）**

![imageJRL8-5-4](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-4.jpg)

QEU:FOUNDER  ： “たぶん、**AIがTRAIN用データから抽出した特徴に何か「普遍的なモノ」がある**んでしょうね。今回は、ソレの追求をやってみたいんです。TRAINデータ群は敢えて簡単・具体的なデータを揃え、TESTデータ群は、より抽象的・複雑なモノを評価したい。”

D先生 ： “なるほどね。でも、前回のプログラムを使って、そんな複雑なことをするのは意味がないでしょう？”

![imageJRL8-5-5](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-5.jpg)

QEU:FOUNDER  ： “だから、今回はファインチューニングをやります。つまり、すでに訓練されたものを使います。ちょうど、使いやすい事例があったので・・・。今回は**Vit-Base**を使いましょう。”

![imageJRL8-5-6](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-6.jpg)

D先生 ： “これらの訓練で使ったIMAGENETっていうデータセットは**1000万件ぐらいの画像集**でしょ？すごい学習量だなぁ・・・。これをファインチューニングすれば、当然、判別精度が良くなりますよね。”

QEU:FOUNDER ： “じゃあ、プログラムをドン！！”

```python
#####
# TRAINING VIT PROGRAM(FLOWER)
#####
import numpy as np
import pandas as pd
from PIL import Image 

# ------
# CLASS NAMES
class_names = ['daisy','dandelion','roses','sunflowers','tulips']

# ------
def create_labels(class_names, all_files):

    # ------
    num_df = len(all_files)

    # ---
    arr_class = []
    arr_index = []
    for row in range(num_df):    # num_df
        name_file = all_files[row]
        for i, str_class in enumerate(class_names):
            if str_class in name_file:
                #print(i, str_class)
                arr_class.append(str_class)
                arr_index.append(i)
    # ---
    np_index = np.array([arr_index]) 

    return num_df, arr_class, np_index

# -----
# TRAIN - VALIDATION
DF_TRAIN = pd.read_csv('training_filesG.csv', dtype='str')
# ---
all_base_paths = DF_TRAIN.loc[:,"base_path"].values
all_type_dirs = DF_TRAIN.loc[:,"type_dir"].values
all_file_names = DF_TRAIN.loc[:,"file_name"].values
# ---
train_base_paths = []
train_type_dirs = []
train_file_names = []
# ---
vali_base_paths = []
vali_type_dirs = []
vali_file_names = []
# ---
for i in range(len(DF_TRAIN)):
    base_path = all_base_paths[i]
    type_dir = all_type_dirs[i]
    file_name = all_file_names[i]
    if 'webp' not in file_name:
        if np.random.rand() > 0.2:
            train_base_paths.append(base_path)
            train_type_dirs.append(type_dir)
            train_file_names.append(file_name)
        else:
            vali_base_paths.append(base_path)
            vali_type_dirs.append(type_dir)
            vali_file_names.append(file_name)
# ---
print(vali_base_paths[0:10])

# -----
# train
num_train, train_class, train_index = create_labels(class_names, train_type_dirs)
print(train_class[0:20])
print(train_index[0,0:20])
print(train_index.shape)

# -----
# validation
num_vali, vali_class, vali_index = create_labels(class_names, vali_type_dirs)
print(vali_class[0:20])
print(vali_index[0,0:20])
print(vali_index.shape)

# -----
# TEST
# -----
DF_TEST = pd.read_csv('testing_filesG.csv', dtype='str')
#print(DF_TEST)
# ---
temp_base_paths = DF_TEST.loc[:,"base_path"].values
temp_type_dirs = DF_TEST.loc[:,"type_dir"].values
temp_file_names = DF_TEST.loc[:,"file_name"].values
# ---
test_base_paths = []
test_type_dirs = []
test_file_names = []
# ---
for i in range(len(DF_TEST)):
    base_path = temp_base_paths[i]
    type_dir = temp_type_dirs[i]
    file_name = temp_file_names[i]  
    if 'webp' not in file_name:
        test_base_paths.append(base_path)
        test_type_dirs.append(type_dir)
        test_file_names.append(file_name)
# -----
# test
num_test, test_class, test_index = create_labels(class_names, test_type_dirs)
print(test_class[0:20])
print(test_index[0,0:20])
print(test_index.shape)

```

![imageJRL8-5-7](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-7.jpg)

QEU:FOUNDER ： “今回のプログラムでは、前回のようにXとYをまとめてデータセットにせず、Xとyが別々になっています。”

D先生 ： “私としては、そちらの方が楽です・・・（笑）。”

```python
###
# CREATE X-Y INPUT DATASET
###
# 画像の読み込み -> ベクトル化
IMAGE_SIZE = 224

# ---
# 画像ベクトルを生成する
def create_vector(base_path, type_dir, file_name, IMAGE_SIZE):

    img = (Image.open(base_path + '/' + type_dir + '/' + file_name)) 
    # Resize
    #img_resize = img.resize((IMAGE_SIZE, IMAGE_SIZE))
    # display the image 
    #display(img_resize)
    #img_arr = np.asarray(img_resize)
    img_arr = np.asarray(img)
    val_shape = img_arr.shape
    #print(img_arr.shape) 
    
    return img_arr, val_shape

# ------
# TRAIN
mx_train = np.array([])
for i in range(num_train):  # num_train
    base_path = train_base_paths[i]
    type_dir = train_type_dirs[i]
    file_name = train_file_names[i]
    # ---
    img_arr, val_shape = create_vector(base_path, type_dir, file_name, IMAGE_SIZE)
    if i == 0:
        mx_train = np.array([img_arr])
    else:
        mx_train = np.concatenate([mx_train, np.array([img_arr])],0)
    if i%100 == 0:
        print(i)
# ---
print(mx_train.shape)

```

**（Xデータを生成した結果）**

![imageJRL8-5-8](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-8.jpg)

QEU:FOUNDER ： “異常が、TRAINのデータの生成です。たった8000件でも、学習用データを作るのに結構時間がかかります。画像のリサイズはすでに済ませているんだけど・・・。”

D先生 ： “PILの画像処理にはGPUで高速化させる機能はないからかな？もし、高速化のアイデアがあれば、ご自分でやってみてください。それでは、つづきに行きましょう。”

```python
# ------
# VALIDATION
mx_vali = np.array([])
for i in range(num_vali):
    base_path = vali_base_paths[i]
    type_dir = vali_type_dirs[i]
    file_name = vali_file_names[i]
    # ---
    img_arr, val_shape = create_vector(base_path, type_dir, file_name, IMAGE_SIZE)
    if i == 0:
        mx_vali = np.array([img_arr])
    else:
        mx_vali = np.concatenate([mx_vali, np.array([img_arr])],0)
    if i%50 == 0:
        print(i)
# ---
print(mx_vali.shape)

# ------
# TEST
mx_test = np.array([])
for i in range(num_test):
    base_path = test_base_paths[i]
    type_dir = test_type_dirs[i]
    file_name = test_file_names[i]
    img_arr, val_shape = create_vector(base_path, type_dir, file_name, IMAGE_SIZE)
    if i == 0:
        mx_test = np.array([img_arr])
    else:
        mx_test = np.concatenate([mx_test, np.array([img_arr])],0)
    if i%50 == 0:
        print(i)
# ---
print(mx_test.shape)

# -----------
# IMPORT PACKAGES
import os
os.environ['PYTHONHASHSEED'] = '0'
import tensorflow as tf
os.environ['TF_DETERMINISTIC_OPS'] = 'true'
os.environ['TF_CUDNN_DETERMINISTIC'] = 'true'
import random as rn

SEED = 123
def reset_random_seeds():
    tf.random.set_seed(SEED)
    np.random.seed(SEED)
    rn.seed(SEED)

reset_random_seeds()    

session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=32, in-ter_op_parallelism_threads=32)
tf.compat.v1.set_random_seed(SEED)
sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)

# ライブラリインポート
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization,Flatten

from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
import tensorflow_addons as tfa

from vit_keras import vit, utils
from sklearn.model_selection import train_test_split

# ---
X_train = mx_train
y_train = train_index[0]
X_valid = mx_vali
y_valid = vali_index[0]
X_test = mx_test
y_test = test_index[0]

```

QEU:FOUNDER ： “ここまでがカスタム（我々固有の）・データセットの生成段階です。ホントに、モダンな機械学習では、判別ロジックの構築よりもデータづくりに手間がかかります。”

D先生 ： “だからこそ、**検査機メーカは（学習）データでお金を儲けられる**んです。”


```python
# 変数定義
image_size = 224 
input_shape=(image_size,image_size,3)

# 予測するクラス数（CIFAR10の場合は10）
num_classes = 5

# -----
# モデル定義(Fine_tuning)
def buildModel_ViT():
    vit_model = vit.vit_b16(
        image_size = image_size,
        activation = 'sigmoid',
        pretrained = True,
        include_top = False,
        pretrained_top = False)
    model = tf.keras.Sequential([
        vit_model,
        tf.keras.layers.Flatten(),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(21, activation = tfa.activations.gelu),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(num_classes, 'softmax')
    ],
    name = 'vision_transformer')
    model.compile(optimizer=optimizers.Adam(learning_rate=1e-4), loss="categorical_crossentropy", met-rics=["accuracy"])
    
    return model

# 訓練用関数の定義
def train_vit_holdout(X_train, X_valid, y_train, y_valid, steps_per_epoch, epochs, batch_size, callbacks):

    # 訓練データと評価データの分割
    y_train = to_categorical(y_train)
    y_valid = to_categorical(y_valid)
    
    # Data Augumentation
    datagen = ImageDataGenerator(rotation_range=20, horizontal_flip=True, zoom_range=0.2)
    train_generator = datagen.flow(X_train, y_train,batch_size=batch_size)
        
    # モデル構築
    model = buildModel_ViT()

    # 訓練(TRAINING)
    history = model.fit(train_generator,
                        steps_per_epoch=steps_per_epoch,
                        epochs=epochs,
                        validation_data=(X_valid, y_valid),
                        callbacks=callbacks,
                        shuffle=True
                       )   
    return model, history

# ----
# 訓練を実行する
batch_size = 16
steps_per_epoch = 165 # trainデータ=2653、バッチサイズ=16なので2653/16=165とした。
epochs = 30 

reduce_lr = ReduceLROnPlateau(monitor='val_accuracy',
                              factor=0.2,
                              patience=2,
                              verbose=1,
                              min_delta=1e-4,
                              min_lr=1e-6,
                              mode='max'
                             )
earlystopping = EarlyStopping(monitor='val_accuracy',
                              min_delta=1e-4,
                              patience=5,
                              mode='max',
                              verbose=1
                             )
callbacks = [earlystopping, reduce_lr]
model, history = train_vit_holdout(X_train, X_valid, y_train, y_valid, steps_per_epoch, epochs, batch_size, callbacks)

# ------
# 学習履歴曲線を描く
# ------
def plot_history(item):
    plt.plot(history.history[item], label=item)
    plt.plot(history.history["val_" + item], label="val_" + item)
    plt.xlabel("Epochs")
    plt.ylabel(item)
    plt.title("Train and Validation {} Over Epochs".format(item), fontsize=14)
    plt.legend()
    plt.grid()
    plt.show()
plot_history("loss")
plot_history("accuracy")

```

**（学習損失の推移）**

![imageJRL8-5-9](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-9.jpg)

**（正確度の推移）**

![imageJRL8-5-10](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-10.jpg)

D先生 ： “さすがに、ViT_Bモデルを使ったファインチューニングの場合、**こんなに少ないデータ量で85％の正解度までいく**んですね。しかも、損失量の大きなアップダウンなく順調に学習しているじゃないですか・・・。”

QEU:FOUNDER ： “さらにびっくりしたことがあるよ。**Validationの正解度がTrainよりも高い**じゃないですか。”

D先生 ： “いままで、こんな現象を見たことがなかったです。やはり、これはファインチューニングならではなんでしょうね。”

```python
# -----
# モデルから予測する
X = X_test
pred = model.predict(X)
print(pred[0:10])

# 予測結果の確認
df_pred = pd.DataFrame(pred)
idx_pred = np.array(df_pred.idxmax(axis=1))
print(idx_pred[0:10])

# -----
df_pred = pd.DataFrame(idx_pred)
df_y = pd.DataFrame(y_test)
df_result = pd.concat([df_y, df_pred], axis=1)
df_result.columns = ['y','pred']
display(df_result)

#------
# 混合行列（Confusion Matrix）の確認
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
print('Confusion Matrix:')
print(confusion_matrix(df_result['y'],df_result['pred']))
print()
print('Accuracy :{:.4f}'.format(accuracy_score(df_result['y'],df_result['pred'])))
print('Precision:{:.4f}'.format(precision_score(df_result['y'],df_result['pred'],average='macro')))
print('Recall   :{:.4f}'.format(recall_score(df_result['y'],df_result['pred'],average='macro')))
print('F_score  :{:.4f}'.format(f1_score(df_result['y'],df_result['pred'],average='macro')))

```

**（判別性能の評価：TESTデータセットを使った）**

![imageJRL8-5-11](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-11.jpg)

QEU:FOUNDER ： “まず言っておくけど、**これは「あのテスト・データセット」での評価結果**だからね。”

D先生 ： “数字を見ると、正解度は大体70％ですね。この簡単な学習で、このレベルのパフォーマンスは、結構立派なものですね。”

QEU:FOUNDER ： “この「抽象化」の能力は実用上は、とても重要なんです。外観検査自動機にViTを使う場合には、TRAINデータのほとんどが3Dレンダリングのモノになります。少なくとも、立ち上げ当初はね。だから、バーチャルのデータを使って、現実の判別をしなければならない。”

D先生 ： “つまり、今回はTrainで写真を使い、Testで絵を評価しました。現実は、その逆であるということですね。このViT技術を検査に対して使うときには、まだまだ工夫がいるんでしょうね。最後に、予測した内容を見てみましょう。”

```python
# ------
# Saving Model
model.save('./files/test_model.h5')

# ------
# 結果を比較する
plt.figure(figsize=(12,12))
icount , vcount = 0, 0
for image,label,tname in zip( X_test[0:1280], idx_pred[0:1280], y_test[0:1280]):
    if icount%80 == 0:
        ax=plt.subplot(4,4,vcount+1)
        plt.imshow(image)
        plt.title("True Label :"+ class_names[tname] + "\n" + "Predicted Label  : " + class_names[label])
        #plt.title("True Label :"+ class_names[labels[i]] + "\n" + "Predicted Label  : " + class_name[tf.argmax(model(tf.expand_dims(images[i],axis=0)),axis= -1).numpy()[0]])
        plt.axis("off")
        vcount = vcount + 1
    icount = icount + 1    
plt.show()
```


**（事例評価：その１）**

![imageJRL8-5-12](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-12.jpg)

QEU:FOUNDER ： “どう、予測の出来栄えは・・・？”

D先生 ： “えっ？いきなり、「どう？」といわれても・・・。確か、デイジーは最も正解が多かった品種でしたよね。”

QEU:FOUNDER ： “じゃあ、他のものも見てみましょう。”

**（事例評価：その２）**

![imageJRL8-5-13](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-13.jpg)

D先生 ： “やっぱり、花の予測は難しいですね。花の場合には、品種間の差異が微妙ですから・・・。DandelionとSunflower、RoseとTulipの区別は意外と大変ですよね。”

QEU:FOUNDER  ： “まあ、今回は、**「なにはともあれファインチューニングを使った！」**というのが大成果です。これをベースとして、さらに遊べます。。”

D先生 ： “何して遊ぶ？”

### [＞寄付のお願い(click here)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

QEU:FOUNDER  ： “いろいろと考えています・・・。ともあれ寄付をください。もっと実用的なものに発展していきます。”


## ～ まとめ ～

### ・・・ 少し前の会話に時計を巻き戻します ・・・

QEU:FOUNDER ： “本当に「品質」がなくなった世界がどうなるかについて、興味ない？この人のこと、覚えてる？”

![imageJRL8-5-14](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-14.jpg)

C部長 : “「品質弁士」といわれる、フィリップおじさんですね・・・（笑）。品質ISO（ISO9001シリーズ）の基本的なしくみは、彼が提案した**ZD（ZERO DEFECT:無欠陥）運動**が起源です。あと、同時（1950～60年代）としては、**「品質とは顧客要求に対する適合(conformance)の程度」**という考え方は斬新でしたね。”

### フィリップ・クロスビー：「品質問題は存在しない。」

QEU:FOUNDER ： “名言が多いんだよね、この人って・・・。その代表例がコレ（↑）・・・。つまり、品質の問題なんかはなく、プロセスの問題が存在するということなんだよね。つまり、この発言は**「プロセス」が存在するサプライアの目線から**なの・・・。だから、プロセスから切り離された（製品が出荷された）ときにはじめて「品質」が発生する。もし、自動検査機があって「ほぼ100％の不良品」が見つかったとした場合、品質の概念ってサプライアにとって必要なの？さらにいえば、最先端の工場（↑）の自動化率は90％を越えたそうだよ。”

![imageJRL8-5-15](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-15.jpg)

C部長 : “そうだ！「出荷した後の消費者が被る損失を品質と定義する」っていうのはどう？”

QEU:FOUNDER ： “それで遊びたいのでしたら、ご自由に・・・（笑）。ちなみに、DX（プロセスのデジタル化）が進んできたら、計算機を回して条件を総当たりすれば最適値が見つかるからね。必要なのは、「誤差因子」という概念だけ・・・。”

C部長 : “じゃあ、や～めた・・・（笑）。”

QEU:FOUNDER ： “別に止めているわけじゃない。「ご自由に」って言っているだけ、「総合SN比」についても、いずれは語りたいと思っているんだから・・・。せっかくだから、**ここで「総合SN比」のコンセプトについても少し議論しましょう**。ちょっと、大変なこと（↓）が起こったから、これを契機に・・・。”

![imageJRL8-5-16](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-16.jpg)

C部長 : “この件には、ひっくりかえるくらい驚きました。・・・なんというか、「BAD TIMING…」・・・。確か、D社の方が品質何某学会の会長になった直後だからねえ・・・。”

QEU:FOUNDER ： “以下、小生が考えている「総合SN比」の基本的な考え方を説明します。ちなみに、今回のD社の品質問題とは直接の関係はないと思いますよ。ココの（↑）記事によると、今回は部品の変形が問題だし・・・。さて、本題・・・。まずは「総合SN比」の元ネタ（ベース概念）である標準SN比をみてみましょう。”

![imageJRL8-5-17](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-17.jpg)

C部長 : “非線形に複雑に動く事象に対して、標準ベクトルを設定します。それを横軸にして出力との関係のグラフを描くと、当然ながらグラフは0点比例式の分布になります。この性質を使って、タグチメソッドのSN比を計測するというやり方です。設計の場合には、必ず設計が想定している「標準」がありますので、標準に対するバラツキをMIN化するためにとても便利なやり方ですね。”

QEU:FOUNDER ： “C先生の解説はお見事！！じゃあ、この考え方を小生なりに大きな図（BIG PICTURE）にします。ドン！！”

![imageJRL8-5-18](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-18.jpg)

QEU:FOUNDER ： “「良いモノ」、例えば**「操作性が良いモノ」というのは線形の特性を持っています**。あるシステムが１の入力に対して10の出力がでるとすると、2の入力では20の出力がでるような・・・。”

![imageJRL8-5-19](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-19.jpg)

C部長 : “実は標準SN比で扱う事例というのも、しょせんは**「若干の非線形」にしかすぎません**。標準SN比を使って最適化した事例というのは、スイッチの肉厚設計が代表ですね。2の入力でいきなり1000の出力になるわけではありません。”

QEU:FOUNDER ： “でもね。2の入力でいきなり1000の出力がでるような「強い非線形」に対して、敢て当てはめることはあり得ますよね。例えば、「燃焼制御」とか・・・。”

C部長 : “ほう・・・。ここで、むりやりD社の件に「当てはめ」ましたね・・・（笑）。でも、実際の話、現在のコンピュータ制御があって初めて、非常に高い燃費が可能となりました。その制御のためには、FOUNDERのいう**「当てはめユニット」**は非常に高度なものであることは、容易に理解できますね。”

![imageJRL8-5-20](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-20.jpg)

QEU:FOUNDER ： “さらに、**今の時代は「ディープラーニング」で当てはめて制御もできます**からね・・・。・・・でもね、そのように高度な当てはめの場合には、必ずリスクも付きまといます。**想定内の誤差因子では、管理限界内で収まるのに対し、誤差因子Bでは管理限界を超える場合も考えられます。**”

C部長 : “複雑な当てはめをシステムに組み込むと、このような事態は容易に考えられます。”

![imageJRL8-5-21](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-21.jpg)

QEU:FOUNDER ： “ここで「総合SN比を議論しましょう」という結論になるわけです。この図は**「標準ベクトル」を「当てはめ関数」に変えてみた**だけ・・・・（笑）。このような異常が発生するような当てはめユニットは、必ず総合SN比が小さくでてくるはずです。”

### SN比 = β^2/σ
### ここで、β：回帰線の傾き、σ回帰線からのプロットのバラツキ

C部長 : “つまり、今回の場合SN比が小さなシステムが悪いということですね。”

QEU:FOUNDER ： “もう一歩うまく考えれば、このSN比つかって改善もできるでしょうね。もうちょっと考えれば、管理限界の形状を変えるようなパラメタ設計もできるし、万が一として当てはめ異常で出力が管理限界を超えたときに社会損失を与えないような安全設計も考えられるでしょ？”

C部長 : “なるほど、面白そうだ・・・。そういえば、FOUNDERは2013年ぐらいから総合SN比を検討していましたよね。「品質のレジリエンス(resilience)」として・・・。”

![imageJRL8-5-22](/2023-12-23-QEUR23_VTRANS4/imageJRL8-5-22.jpg)

QEU:FOUNDER ： “よっ！覚えていてくれてありがとう！！あの当時は、小生はまだディープラーニングを知らなかったから、総合SN比の意義が不明確だった。これからは、総合SN比の概念は重要になると思います。”

C部長 : “**将来的に品質という言葉がなくなり、「レジリエンス」に入れかわるかも・・・（笑）。**”

