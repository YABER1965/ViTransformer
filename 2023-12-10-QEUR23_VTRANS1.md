---
title: QEUR23_VTRANS1: 手書き文字の学習(CHANNEL=1)
date: 2023-12-10
tags: ["QEUシステム", "メトリックス", "Python言語", "Vision Transformer", "LLM", "データセット", "Fine-tuning", "イノベーション"]
excerpt: Vision Transformer(ViT)をやってみる
---

## QEUR23_VTRANS1: 手書き文字の学習(CHANNEL=1)

### ～ まずは、最も簡単なものから始めましょう ～

C部長 ： “それでは、始めましょう。まずは、コレ（↓）をネタに議論したいんですが・・・。”

![imageJRL8-2-1](/2023-12-10-QEUR23_VTRANS1/imageJRL8-2-1.jpg)

QEU:FOUNDER ： “おっ、今回は気合が入っていますね！いいよ、イイよ・・・。”

C部長： “「もう、QEやらなくていいんだぁ・・・。」と思うと、なぜかムラムラと創造力が・・・(笑)。”

QEU:FOUNDER ： “QE（品〇工学）って、もともと我々はほとんどやらなかったじゃない。やっても、やったことといえばT法とRT法がらみだけですから・・・。さらにいうと、ひょっとしたら今後は使うことがあるかもよ。こんな感じ（↓）で・・・。”

![imageJRL8-2-2](/2023-12-10-QEUR23_VTRANS1/imageJRL8-2-2.jpg)

D先生 ： “これは、ViT用のトランスフォーマーの構成ですよね。”

QEU:FOUNDER ： “インプットの部分に**「Embedding」**ってあるよね。この部分を自由に変更できるわけです。CNNのように畳み込みを適用すると、認識の性能が上がるらしい・・・。”

D先生 ： “つまり、「今後もご縁があるかも」ということ。ただし、**万が一**ですが・・・(笑)。C大先生、良い記事をせっかく読んだんだから、ちょっと内容を紹介してください。”

- ViT demonstrated its effectiveness for the CV tasks; the vision transformers have received considerable attention and undermined the dominance of CNNs in the CV field.
ViT は CV タスクに対する有効性を実証しました。 ビジョントランスフォーマーはかなりの注目を集めており、CV 分野における CNN の優位性が損なわれています。
- Since Transformers require a large amount of data for high accuracy, the data collection process can extend project time. In the case of having less data, CNNs generally perform better than Transformers.
Transformers は高精度を実現するために大量のデータを必要とするため、データ収集プロセスによりプロジェクト時間が長くなる可能性があります。 データが少ない場合、CNN は通常、Transformer よりも優れたパフォーマンスを発揮します。
- The training time of the Transformer takes less than CNNs. Comparing them in terms of computational efficiency and accuracy, Transformers can be chosen if the time for model training is limited.
Transformer のトレーニング時間は CNN よりも短くなります。 計算効率と精度の観点からこれらを比較すると、モデルのトレーニング時間が限られている場合は、Transformer を選択できます。
- The self-attention mechanism can bring more awareness to the developed model. Since it is so hard to understand the weaknesses of the model developed by CNNs, attention maps can be visualized, and they can help developers to guide how to improve the model. This process is harder for CNN-based models.
自己注意メカニズムにより、開発されたモデルにより多くの認識をもたらすことができます。 CNN によって開発されたモデルの弱点を理解するのは非常に難しいため、アテンション マップは視覚化でき、開発者がモデルを改善する方法をガイドするのに役立ちます。 このプロセスは、CNN ベースのモデルではより困難です。
- Deployment of chosen approach should be straightforward and fast to get ready to be deployed (If you do not have time limits, no problem). Even though there are some frameworks for Transformers, CNN-based approaches are still less complex to be deployed.
選択したアプローチの導入は、簡単かつ迅速に導入準備を完了できる必要があります (時間制限がない場合は問題ありません)。 Transformer 用のフレームワークはいくつかありますが、CNN ベースのアプローチはまだ導入がそれほど複雑ではありません。
- The emergence of Vision Transformers has also provided an important foundation for developing vision models. The largest vision model is Google’s ViT-MoE model, which has 15 billion parameters. These large models have set new records on the ImageNet-1K classification.
ビジョン トランスフォーマーの出現により、ビジョン モデルを開発するための重要な基盤も提供されました。 最大のビジョン モデルは Google の ViT-MoE モデルで、150 億のパラメータがあります。 これらの大型モデルは、ImageNet-1K 分類における新記録を樹立しました。

D先生 ： “へぇ～、ViTって、すごいんですね。あの一世を風靡したCNNが劣勢になる時代がくるとは思いませんでした・・・。でも、**学習データがたくさん必要になる**のは大変ですよね。”

QEU:FOUNDER ： “ただし、トランスフォーマーの場合、**Fine-Tuning**ができるので、既存のデータセットを持ってくれば問題は少ないですよ。何なら、G社の学習済みのモデルを使って、それをチューニングしてもいいです。”

C部長： “なるほど・・・。だから、QEは忘れちゃえといったのか。もし、ロジックを変えると、**「学びなおし」が発生して大変なことになる**から・・・。”

QEU:FOUNDER ： “そう。そういうことです。小生は、まずは普通に画像判別をおこない、それを検査に発展させるとどうなるかをしりたい。さしあたりはそれだけ・・・。”

D先生 ： “それにしても、FOUNDERも、発想が**「目的指向」**になりましたね。”

![imageJRL8-2-3](/2023-12-10-QEUR23_VTRANS1/imageJRL8-2-3.jpg)

QEU:FOUNDER ： “あたりまえだろ！十何年もまえから目的（↑）がはっきりしてるから、**「Start from Automatic Inspection」**って・・・。じゃあ、具体的に、第一歩をはじめますか。参考動画をドン！”

[![MOVIE1](http://img.youtube.com/vi/Vonyoz6Yt9c/0.jpg)](http://www.youtube.com/watch?v=Vonyoz6Yt9c "Implement and Train ViT From Scratch for Image Recognition")

D先生 ： “テーマが「手書き文字の判別」ですよね。いままで、さんざんやってみた問題です。”

QEU:FOUNDER ： “この事例では、「前処理」として畳み込みを使っているようです。**PatchEnbedding**のクラスをみてみましょう。”

```python

PatchEmbedding
class PatchEmbedding(nn.Module):
    def __init__(self, embed_dim, patch_size, num_patches, dropout, in_channels):
        super().__init__()
        self.patcher = nn.Sequential(
            nn.Conv2d(
                in_channels=in_channels,
                out_channels=embed_dim,
                kernel_size=patch_size,
                stride=patch_size,
            ),
            nn.Flatten(2))

        self.cls_token = nn.Parameter(torch.randn(size=(1, in_channels, embed_dim)), requires_grad=True)
        self.position_embeddings = nn.Parameter(torch.randn(size=(1, num_patches+1, embed_dim)), re-quires_grad=True)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, x):
        cls_token = self.cls_token.expand(x.shape[0], -1, -1)

        x = self.patcher(x).permute(0, 2, 1)
        x = torch.cat([cls_token, x], dim=1)
        x = self.position_embeddings + x
        x = self.dropout(x)
        return x

```

C部長： “おお・・・。**「nn.Conv2d」**と書いてある・・・。”

QEU:FOUNDER ： “別に、この部分を「RT」に変換しても問題ないですよ。実際は、こんなことを考えてたの・・・。興味があれば、ご自分でやってみてください。それでは、コードの紹介です。これは引用にすぎないので、途中は省略します。”

```python

# -----
!pip install torch torchvision
!pip install scikit-learn
!pip install tqdm

# vit-implementation_hand-writing
# import package
import torch
import pandas as pd
from torch import nn
from torch import optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import random
#import timeit
from tqdm import tqdm

# Parameter
RANDOM_SEED = 42
BATCH_SIZE = 512
EPOCHS = 40
LEARNING_RATE = 1e-4
NUM_CLASSES = 10
PATCH_SIZE = 4
IMG_SIZE = 28
IN_CHANNELS = 1
NUM_HEADS = 8
DROPOUT = 0.001
HIDDEN_DIM = 768
ADAM_WEIGHT_DECAY = 0
ADAM_BETAS = (0.9, 0.999)
ACTIVATION="gelu"
NUM_ENCODERS = 4
EMBED_DIM = (PATCH_SIZE ** 2) * IN_CHANNELS # 16
NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2 # 49

random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
torch.cuda.manual_seed(RANDOM_SEED)
torch.cuda.manual_seed_all(RANDOM_SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

device = "cuda" if torch.cuda.is_available() else "cpu"


# ------
# Patch Embedding
class PatchEmbedding(nn.Module):
    def __init__(self, embed_dim, patch_size, num_patches, dropout, in_channels):
        super().__init__()
        self.patcher = nn.Sequential(
            nn.Conv2d(
                in_channels=in_channels,
                out_channels=embed_dim,
                kernel_size=patch_size,
                stride=patch_size,
            ),                  
            nn.Flatten(2))
    
# 以下省略
    
model = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS).to(device)
x = torch.randn(512, 1, 28, 28).to(device)
print(model(x).shape)

# ------
# ViT(Vision Transformer)
class ViT(nn.Module):
    def __init__(self, num_patches, img_size, num_classes, patch_size, embed_dim, num_encoders, num_heads, hidden_dim, dropout, activation, in_channels):
        super().__init__()
        self.embeddings_block = PatchEmbedding(embed_dim, patch_size, num_patches, dropout, in_channels)
        
        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, drop-out=dropout, activation=activation, batch_first=True, norm_first=True)
        self.encoder_blocks = nn.TransformerEncoder(encoder_layer, num_layers=num_encoders)

        self.mlp_head = nn.Sequential(
            nn.LayerNorm(normalized_shape=embed_dim),
            nn.Linear(in_features=embed_dim, out_features=num_classes)
        )
    
# 以下省略

model = ViT(NUM_PATCHES, IMG_SIZE, NUM_CLASSES, PATCH_SIZE, EMBED_DIM, NUM_ENCODERS, NUM_HEADS, HIDDEN_DIM, DROPOUT, ACTIVATION, IN_CHANNELS).to(device)
x = torch.randn(512, 1, 28, 28).to(device)
print(model(x).shape) # BATCH_SIZE X NUM_CLASSES

# ------
# Read datasets
train_df = pd.read_csv("./train.csv")
test_df = pd.read_csv("./test.csv")
submission_df = pd.read_csv("./sample_submission.csv")

# 以下省略

```

D先生 ： “今回のデータはCSVに入っているんですね。28x28の画像データですか・・・。トレーニングデータは何件ですか？”

![imageJRL8-2-4](/2023-12-10-QEUR23_VTRANS1/imageJRL8-2-4.jpg)

QEU:FOUNDER ： “プログラムを回して、自分で調べてください。たしか、レコードは60000件位じゃないかな？知らんけど・・・（笑）。”

```python

# ---
train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=RANDOM_SEED, shuffle=True)

# ------
# Train dataset
class MNISTTrainDataset(Dataset):
    def __init__(self, images, labels, indicies):
        self.images = images
        self.labels = labels
        self.indicies = indicies
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.RandomRotation(15),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])
        ])
        
# 以下省略
    
class MNISTValDataset(Dataset):
    def __init__(self, images, labels, indicies):
        self.images = images
        self.labels = labels
        self.indicies = indicies
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])
        ])
    
# 以下省略

class MNISTSubmitDataset(Dataset):
    def __init__(self, images, indicies):
        self.images = images
        self.indicies = indicies
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])
        ])
    
# 以下省略

# ------
# Display Input
plt.figure()
f, axarr = plt.subplots(1, 3)

train_dataset = MNISTTrainDataset(train_df.iloc[:, 1:].values.astype(np.uint8), train_df.iloc[:, 0].values, train_df.index.values)
print(len(train_dataset))
print(train_dataset[0])
axarr[0].imshow(train_dataset[0]["image"].squeeze(), cmap="gray")
axarr[0].set_title("Train Image")
print("-"*30)

val_dataset = MNISTValDataset(val_df.iloc[:, 1:].values.astype(np.uint8), val_df.iloc[:, 0].values, val_df.index.values)
print(len(val_dataset))
print(val_dataset[0])
axarr[1].imshow(val_dataset[0]["image"].squeeze(), cmap="gray")
axarr[1].set_title("Val Image")
print("-"*30)

test_dataset = MNISTSubmitDataset(test_df.values.astype(np.uint8), test_df.index.values)
print(len(test_dataset))
print(test_dataset[0])
axarr[2].imshow(test_dataset[0]["image"].squeeze(), cmap="gray")
axarr[2].set_title("Test Image")
print("-"*30)

plt.show()

# ------
# DataLoaders
train_dataloader = DataLoader(dataset=train_dataset,
                              batch_size=BATCH_SIZE,
                              shuffle=True)

val_dataloader = DataLoader(dataset=val_dataset,
                            batch_size=BATCH_SIZE,
                            shuffle=True)

test_dataloader = DataLoader(dataset=test_dataset,
                             batch_size=BATCH_SIZE,
                             shuffle=False)

```

QEU:FOUNDER： “久しぶりにみたわ、手書き文字・・・。”

![imageJRL8-2-5](/2023-12-10-QEUR23_VTRANS1/imageJRL8-2-5.jpg)

```python

# ------
# Learn
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), betas=ADAM_BETAS, lr=LEARNING_RATE, weight_decay=ADAM_WEIGHT_DECAY)

#start = timeit.default_timer()
for epoch in tqdm(range(EPOCHS), position=0, leave=True):
    model.train()
    train_labels = []
    train_preds = []
    train_running_loss = 0
    for idx, img_label in enumerate(tqdm(train_dataloader, position=0, leave=True)):
        img = img_label["image"].float().to(device)
        label = img_label["label"].type(torch.uint8).to(device)
        y_pred = model(img)
        y_pred_label = torch.argmax(y_pred, dim=1)

        train_labels.extend(label.cpu().detach())
        train_preds.extend(y_pred_label.cpu().detach())
        
        loss = criterion(y_pred, label)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_running_loss += loss.item()
    train_loss = train_running_loss / (idx + 1)

#以下省略

```

QEU:FOUNDER： “ここは学習の初期段階です。”

D先生 ： “さすがに、損失は大きくでますね。当たり前ですが・・・。”

**(START)**

![imageJRL8-2-6](/2023-12-10-QEUR23_VTRANS1/imageJRL8-2-6.jpg)

QEU:FOUNDER： “そして、学習の終わりです。少しづつ良くなってきてますが・・・。”

D先生 ： “あれ？そんなに、ViTは能書きのわりに正解率が高くないですよね？”

**(END)**

![imageJRL8-2-7](/2023-12-10-QEUR23_VTRANS1/imageJRL8-2-7.jpg)

```python

# ------
# Display result
labels = []
ids = []
imgs = []
model.eval()
with torch.no_grad():
    for idx, sample in enumerate(tqdm(test_dataloader, position=0, leave=True)):
        img = sample["image"].to(device)
        ids.extend([int(i)+1 for i in sample["index"]])
        
        outputs = model(img)
        
        imgs.extend(img.detach().cpu())
        labels.extend([int(i) for i in torch.argmax(outputs, dim=1)])

# ---
plt.figure()
f, axarr = plt.subplots(2, 3)
counter = 0
for i in range(2):
    for j in range(3):
        axarr[i][j].imshow(imgs[counter].squeeze(), cmap="gray")
        axarr[i][j].set_title(f"Predicted {labels[counter]}")
        counter += 1

# ---
submission_df = pd.DataFrame(list(zip(ids, labels)),
               columns =["ImageId", "Label"])
submission_df.to_csv("submission.csv", index=False)
submission_df.head()

```

QEU:FOUNDER： “判別結果は、以下のように出ました。”

![imageJRL8-2-8](/2023-12-10-QEUR23_VTRANS1/imageJRL8-2-8.jpg)

D先生 ： “「98％は軽く行くのかな？」と思っていました。データが足らないのか？学習不足なのか・・・。”

QEU:FOUNDER ： “ハイパー・パラメタ(Hyper Parameter)を変えて、自分で遊んでみればわかってきますよ。”

C部長： “でも、仮に60000件のトレーニング・データがあって、それで「学習データが不足です」となったら、結構ショックだ・・・。”

D先生 ： “まあ、なんとかなるとは思います。Manufacturing（人工物）の世界ならばね・・・。3Dレンダリングマシンを回せば、山ほどデータが出てきます。”

QEU:FOUNDER ： “ただし、自分が検査したい対象物のみを学習すればよいというわけじゃないんですよ。CNNやViTの場合・・・。・・・というわけで次回につづく。カンパをください・・・。”

### [＞寄付のお願い(click here)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

D先生 ： “つぎは、もう少しレベルの高い事例をやりたいです。”

## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “本当に**「品質がなくなった世界」**がどうなるかについて、興味ない？”

### おっさん（＠QCサークル講話）；「従業員の皆さんにはテレビを見てください。皆が同じように考えてください。」

C部長 : “組織がTQM（ハラ〇メント）にかけていた人力、そして従業員が本来もつ能力を**創造に向けられる**とどうなるんでしょうね？”

QEU:FOUNDER ： “品質にかかわる学問をやるんだったら、本来、こんなことを議論してほしいよね。もう、世の中も大きく変わったことだし。例えば、こんな感じで・・・。この人(↓)、覚えてる？”

![imageJRL8-2-9](/2023-12-10-QEUR23_VTRANS1/imageJRL8-2-9.jpg)

C部長 : “知っていますよ！「デミ〇グ賞」の人でしょ？”

QEU:FOUNDER ： “じゃあ、この人は・・・？”

![imageJRL8-2-10](/2023-12-10-QEUR23_VTRANS1/imageJRL8-2-10.jpg)

C部長 : “**「品質弁士」**といわれる、フィリップおじさんですね・・・（笑）。”

### フィリップ・クロスビー：「品質問題は存在しない。」

QEU:FOUNDER ： “名言が多いんだよね。この人・・・。その代表例がコレ（↑）・・・。つまり、「品質の問題なんかはなく、プロセスの問題が存在する」ということなんだよね。つまり、認識の範囲内に「プロセス」が存在する、サプライアの目線からなの・・・。だから、プロセスから切り離された（製品が出荷された）ときにはじめて「品質」が発生する。”

C部長 : “結局、何が言いたい？”

![imageJRL8-2-11](/2023-12-10-QEUR23_VTRANS1/imageJRL8-2-11.jpg)

QEU:FOUNDER ： “もし、自動検査機があって「ほぼ100％の不良品」が見つかったとした場合、品質の概念ってサプライアにとって必要なの？さらにいえば、最先端の工場（↑）の自動化率は90％を越えたそうだよ。”

[![MOVIE2](http://img.youtube.com/vi/CKjMRPvvs_Y/0.jpg)](http://www.youtube.com/watch?v=CKjMRPvvs_Y "【独占】元タカラジェンヌ東小雪さん衝撃の告白。宝塚歌劇団全員が被害者となり加害者となる恐るべき構造。安冨歩東大教授")

C部長 : “あ～あ、TQMねえ・・・。そうだ！「出荷した後の消費者が被る損失を品質と定義する」っていうのはどう？”

QEU:FOUNDER ： “それで遊びたいのでしたら、ご自由に・・・（笑）。ちなみに、DX（プロセスのデジタル化）が進んできたら、計算機を回して条件を総当たりすれば最適値が見つかるからね。**必要なのは、「誤差因子」という概念だけ・・・。**”

C部長 : “じゃあ、や～めた・・・（笑）。”

QEU:FOUNDER ： “別に止めているわけじゃない。「ご自由に」って言っているだけ、**「総合SN比」**についても、いずれは語りたいと思っているんだから・・・。もう、世の中も大きく変わったことだし。ホレ・・・。C部長・・・。オタクの会社、まだ**「技能実習生」**とやらを使っているの・・・？”

![imageJRL8-2-12](/2023-12-10-QEUR23_VTRANS1/imageJRL8-2-12.jpg)

C部長 : “V国からの実習生がかえっちゃいました。ああ、そういうことだったのね・・・。**J国みたいな遅れた国には「バカバカ」しくって、もういられないと・・・。**”

QEU:FOUNDER ： “これから起こることは、それどころじゃないよ・・・。”

![imageJRL8-2-13](/2023-12-10-QEUR23_VTRANS1/imageJRL8-2-13.jpg)

C部長 : “ハハ・・・、オワタ・・・（笑）。”

### オッサン（海外工場のあいさつにて）：「私の使命はこの会社で終身雇用制を実現することにある・・・。」

QEU:FOUNDER ： “小生が、おっさんの**「のんきなスピーチ」**を聞いて、血圧が上がって倒れそうになったのは10年前か・・・。”

C部長 : “あれ？このコメントもFOUNDER発なの？”

QEU:FOUNDER ： “おっと、またバレた・・・（笑）。あの頃は、**「自分（達）の未来に対する危機感」**をQEで解決しようと、昼は仕事、真夜中からQEの新手法の開発をやっていて、毎日がフラフラな状態になっていた。”

C部長 : “ほう・・・。あれはROUND1-5のころぐらいですね。そのころは、新手法の開発を、まだ統計の延長としていたんですよね。それなりに成果は出ましたが・・・。QEを拡張するために**「汎用技術構造」**の概念を開発したとか・・・。”

QEU:FOUNDER ： “そういう意味では、QEUシステムの開発の歴史って、それなりにご時世にキャッチアップしているんです。ちょっと、ホントのところ最先端にはちょっと遅かったが、小生のアンテナは昔は「この程度」だったので、まあ、しょうがないよね。”
