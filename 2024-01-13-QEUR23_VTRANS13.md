---
title: QEUR23_VTRANS13: SOART3メトリックスによる合成画像の生成
date: 2024-01-13
tags: ["QEUシステム", "メトリックス", "Python言語", "Vision Transformer", "LLM", "データセット", "Fine-tuning", "イノベーション"]
excerpt: Vision Transformer(ViT)をやってみる
---

## QEUR23_VTRANS13: SOART3メトリックスによる合成画像の生成

## ～ 変わるよ変わる、SOARTメトリックス ～

### ・・・ 前回の続きです ・・・

D先生 ： “ほう・・・。今回は、撮影用のカメラを3台設置しましたね。そして、中央カメラが1番になると・・・。・・・となると、これからの展開は前回と同じ解析手法（SOART法）になるの？。”

![imageJRL8-14-1](/2024-01-13-QEUR23_VTRANS13/imageJRL8-14-1.jpg)

C部長 : “どうなんです？”

QEU:FOUNDER ： “小生は淡々とプロジェクトを進めるだけ・・・（笑）。まずはセンター・カメラの平均画像をつくらなきゃね。それでは、プログラムをドン！”

```python
# -------------------- プログラムの始まり -------------------
# -*- coding: utf-8 -*-
# filename: create_average_pictures.py
# 平均画像を生成する
# ---------------------------------------------------
# ライブラリをインポートする
# OPENCV関連
import cv2
import os, sys, time

# 数値計算
import csv
from datetime import datetime
from PIL import Image
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt  # データプロット用ライブラリ

#=================================================
# READ CSV_FILE FUNCTION
#=================================================
# ファイルを読み込み表示する
def read_csvfile(file_readcsv): 
 
    # 畳み込みファイルの読み込み
    df = pd.read_csv(file_readcsv) 
    #dfC = df[df["rtm"] == 'tani']
    arr_pics = df.loc[:,"file_name"].values
    len_namPics = len(arr_pics)
  
    return len_namPics, df, arr_pics
    
#=================================================
# difinition of function
#=================================================
# 画像の平均化処理
def ave_process(ave_pics,img_gray,len_mean):

    # 平均画像を描く
    for iRow in range(pic_height):
        for jCol in range(pic_width):
            ave_pics[iRow,jCol] = ave_pics[iRow,jCol] + img_gray[iRow,jCol]/len_mean

    return ave_pics

#=================================================
# Calculation class
#=================================================
# 画像処理（前処理用）
class pics_prepro():

    def __init__(self):
        
        # CSVファイルの読み込み
        file_cnv_input = foldername + "labels0_OK_center.csv"  # ファイルパス名の生成 
        len_namPics, dfC, arr_pics = read_csvfile(file_cnv_input)
        print(dfC)
        #print(arr_pics)

        # 画像ファイル(全体)の読み込みと平均化
        #self.read_pictures(len_namPics, arr_pics)

    # 画像ファイル(全体)の読み込みと平均化
    def read_pictures(self, len_namPics, arr_pics):

        # --------------------------------------------------
        # 初期化処理
        ave_pics = np.zeros([pic_height, pic_width])
        mx_mpics = np.zeros([pic_height, pic_width])
        
        # --------------------------------------------------
        # 画像の平均化処理
        for iCnt_file in range(len_namPics):   # len_namPics

            # 原画像の読み込み
            readpic_name =  foldername + arr_pics[iCnt_file] + ".png"
            img = cv2.imread(readpic_name, flags=cv2.IMREAD_COLOR)
            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 

            # 画像の平均化処理
            ave_pics = ave_process(ave_pics, img_gray, len_namPics)   # len_namPics

        # 整数化への変換
        for iRow in range(pic_height):
            for jCol in range(pic_width):
                mx_mpics[iRow,jCol] = int(ave_pics[iRow,jCol])

        # 結果配列の出力
        print("--- ave_pics ---")
        print(mx_mpics.shape)
        print(mx_mpics)

        # -----
        # 平均画像ファイルの出力
        file_picout = foldername + "average_pic.png"
        plt.title('result--{0}'.format(file_picout))
        plt.imshow(mx_mpics)
        plt.show()

        # 平均画像ファイルの出力
        cv2.imwrite(file_picout, mx_mpics)

#=================================================
# main function            
#=================================================
if __name__ == "__main__":

    # ---------------------------
    # フォルダ名
    foldername = "./"

    # ---------------------------
    # 画像の高さと幅
    pic_height, pic_width = 1080, 1920

    # ---------------------------
    # 画像の平均処理
    pics_prepro()
```

QEU:FOUNDER ： “結果は以下の画像です。Cさんも見覚えがあるでしょ？”

![imageJRL8-14-2](/2024-01-13-QEUR23_VTRANS13/imageJRL8-14-2.jpg)

C部長 : “なんだ。これは以前と同じプログラムです。・・・ということは、**ViT(Vision Transformer)による異常検出もモノクロ画像でやってしまう**わけですね。”

QEU:FOUNDER ： “さあ、それはどうかな？つぎは、おまたせのSOART3メトリックスをつかった合成画像の生成プログラムです。センター・カメラの平均画像を入力、さらにレフトとライトのカメラの画像を順次入力します。”

```python
# -------------------- プログラムの始まり -------------------
# -*- coding: utf-8 -*-
# filename: create_pic_via_SOART3.py
# SOART3によるDOUBLE_EYE可視化分析用
# ---------------------------------------------------
# モジュールのインポート
import cv2
from fastai.vision.all import *
import pandas as pd
import numpy as np

#=================================================
# SUB PROGRAM : 画像ファイルを読み込む、パラメタを指定する
#=================================================
# 標準画像を読み込み表示する
average_pic = "average_pic.png"
# ---
left_pics = ["NA"]*4
left_pics[0] = "./camera_test/31D05/camera0_0_326_399_89_0_0_NUKE31NG"
left_pics[1] = "./camera_test/41D05/camera0_0_398_432_89_0_0_NUKE41NG"
left_pics[2] = "./camera_test/31X10/camera0_0_384_438_89_0_0_BEND31NG"
left_pics[3] = "./camera_test/41X10/camera0_0_489_468_89_0_0_BEND41NG"
# ---
right_pics = ["NA"]*4
right_pics[0] = "./camera_test/31D05/camera0_2_326_399_89_0_0_NUKE31NG"
right_pics[1] = "./camera_test/41D05/camera0_2_398_432_89_0_0_NUKE41NG"
right_pics[2] = "./camera_test/31X10/camera0_2_384_438_89_0_0_BEND31NG"
right_pics[3] = "./camera_test/41X10/camera0_2_489_468_89_0_0_BEND41NG"

# ------
# 標準画像を読み込む, サイズ：(1080, 1920)
filenameC = average_pic
imgC = cv2.imread(filenameC, cv2.IMREAD_GRAYSCALE)
#imgC.shape
# ------
# 画像リサイズのパラメタ
oldsize = (1080, 1920)
# ストライド量
stride_tsr_row, stride_tsr_col = 4, 4
# RT法処理のサイズ
crtm_tsr_row, crtm_tsr_col = 4, 4
# ------
# テンソル行列の処理(for)開始点のベクトルを生成する
row_range, col_range = [], []
sum_row = 0
while sum_row + crtm_tsr_row <= oldsize[0]:
    row_range.append(sum_row)
    sum_row  += stride_tsr_row
# -----
sum_col = 0
while sum_col + crtm_tsr_col <= oldsize[1]:
    col_range.append(sum_col)
    sum_col  += stride_tsr_col
# -----
print("--- row_range ---")
print(row_range)

# ------
# 計算する画像番号を指定する
image_number = 0
# ------
# 左右画像を読み込む, サイズ：(1080, 1920)
# ------
# （左側）
filenameL = left_pics[image_number] + ".png"
print(filenameL)
imgL = cv2.imread(filenameL, cv2.IMREAD_GRAYSCALE)
# ------
# （右側）
filenameR = right_pics[image_number] + ".png"
print(filenameR)
imgR = cv2.imread(filenameR, cv2.IMREAD_GRAYSCALE)

#=================================================
# MAIN PROGRAM : マトリックスの生成と合成画像の表示
#=================================================
# インスタンス化
L1_loss  = torch.nn.L1Loss()
MSE_loss = torch.nn.MSELoss()

# ---------------------------
# soaRT3メトリックスを計算する(テンソル活用版)
def calc_soaRT3(L1_loss, MSE_loss, tsr_sig_array, tsr_tani_array): 

    # データの抽出
    y = tsr_sig_array
    x = tsr_tani_array
    #print(y)
    #print(x)

    # dot回転を計測
    xx = np.dot(x,x) + 0.0001
    xy = np.dot(x,y) + 0.0001
    beta = xy/xx

    # 二乗距離（ユークリッド）を計測
    vDistance = np.linalg.norm(y-beta*x)

    # 絶対値距離（マンハッタン）を計測
    mDistance = np.linalg.norm(y - beta*x, ord=1)
    #print("mDistance: ", mDistance.item())
    
    # 値の変換
    log_beta  = math.log(beta)
    log_yita = math.log(mDistance+1.0)
    log_gamma = math.log(vDistance+1.0) - log_yita
    
    return log_beta, log_yita, log_gamma

# ----
max_row = len(row_range)
max_col = len(col_range)
mx_soart = np.zeros([max_row, max_col, 3])
# ---
for i, row in enumerate(row_range):
    for j, col in enumerate(col_range):

        cut_imgC = imgC[row:row+crtm_tsr_row,col:col+crtm_tsr_col]
        cut_imgL = imgL[row:row+crtm_tsr_row,col:col+crtm_tsr_col]
        cut_imgR = imgR[row:row+crtm_tsr_row,col:col+crtm_tsr_col]
        # ---
        arr_imgC = cut_imgC.flatten()/255
        arr_imgL = cut_imgL.flatten()/255
        arr_imgR = cut_imgR.flatten()/255
        
        # ---
        # 左側画像の分析
        betaL, yitaL, gammaL = calc_soaRT3(L1_loss, MSE_loss, arr_imgL, arr_imgC)
        # 右側画像の分析
        betaR, yitaR, gammaR = calc_soaRT3(L1_loss, MSE_loss, arr_imgR, arr_imgC)             
        mx_soart[i,j,0] = betaL - betaR
        mx_soart[i,j,1] = yitaL - yitaR
        mx_soart[i,j,2] = gammaL - gammaR

# ------
# 各チャンネルの最大値、最小値を設定する
val_max_layer0 = np.max(mx_soart[:,:,0])
val_min_layer0 = np.min(mx_soart[:,:,0])
print(val_max_layer0, val_min_layer0)
# 2.851185143598773 -2.8371004262608954
# ---
val_max_layer1 = np.max(mx_soart[:,:,1])
val_min_layer1 = np.min(mx_soart[:,:,1])
print(val_max_layer1, val_min_layer1)
# 0.2746640556063528 -0.2898955645773872
# ---
val_max_layer2 = np.max(mx_soart[:,:,2])
val_min_layer2 = np.min(mx_soart[:,:,2])
print(val_max_layer2, val_min_layer2)
# 0.1787364221551161 -0.17521090163236316
# ------
norm_soart = mx_soart.copy()
norm_soart[:,:,0] = (mx_soart[:,:,0]-val_min_layer0)/(val_max_layer0-val_min_layer0)
norm_soart[:,:,1] = (mx_soart[:,:,1]-val_min_layer1)/(val_max_layer1-val_min_layer1)
norm_soart[:,:,2] = (mx_soart[:,:,2]-val_min_layer2)/(val_max_layer2-val_min_layer2)
print(norm_soart[100:120,100:120,0])

# ------
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

# SOART3合成画像の出力
# 標準画像
print("標準画像:{}".format(filenameC))
# （左側）
print("左側画像:{}".format(filenameL))
# （右側）
print("右側画像:{}".format(filenameR))
# arrayを画像出力に適した型式に変換する
im_soart = np.asarray(norm_soart)
# ノートブック上に画像を貼り付けする
plt.imshow(im_soart)

```

QEU:FOUNDER ： “さあ、おまたせのSOART3メトリックスの合成画像です。”

**（SOART3合成画像）**

![imageJRL8-14-3](/2024-01-13-QEUR23_VTRANS13/imageJRL8-14-3.jpg)

QEU:FOUNDER ： “一本の端子が垂直に抜けたコネクタの画像であり、以下の左右カメラからとった画像を使用しています。”

**（左カメラの画像）**

![imageJRL8-14-4](/2024-01-13-QEUR23_VTRANS13/imageJRL8-14-4.jpg)

**（右カメラの画像）**

![imageJRL8-14-5](/2024-01-13-QEUR23_VTRANS13/imageJRL8-14-5.jpg)

C部長 ： “あれ？いつの間にか、**グレースケールの画像がカラーの画像になった**・・・。 “

![imageJRL8-14-6](/2024-01-13-QEUR23_VTRANS13/imageJRL8-14-6.jpg)

QEU:FOUNDER ： “SOART3処理による合成画像になったら、なぜかコネクタ・ハウジングが消えちゃったでしょ？この部分は端子抜け検査に不要だったので消えてしまったんです。ここで、SOART3のロジックを少し改造してみます。γ(gamma)メトリックの定義を変えてみました。”

```python
    # 体積ひずみを計測
    xx = torch.dot(x,x) + 0.0001
    yy = torch.dot(y,y) + 0.0001
    gamma = yy/xx.item()
```

C部長 : “画像は、前回とかなり似ていますが、より**「グレースケール」**に近づきました。”

**(NO0)**

![imageJRL8-14-7](/2024-01-13-QEUR23_VTRANS13/imageJRL8-14-7.jpg)

QEU:FOUNDER ： “SOARTのγメトリックスとしては、小生は当初は「体積ひずみ(yy/xx)」を考えていました。D先生、ユークリッド差分値と体積ひずみ値のどちらをγ値にすることが、画像認識としていいと思う？”

D先生 ： “グレースケールって情報量が少ないですよね。・・・であるならば、**より画像がカラフルになるロジックが良い**のではないでしょうか。 “

QEU:FOUNDER ： “じゃあ、考え方をもっと進めて、**チェビシェフ距離（ベクトル要素の最大差異を取る）**の方が良いかもしれないね。チェビシェフ距離はベクトルの比較結果がより極端にでてきます。ドン・・・。”

```python
    # チェビシェフ距離を計測
    vDistance = chebyshev(y,beta*x)
```

QEU:FOUNDER ： “計算すると、こんな画像（↓）が出力されました。ユーグリッド差分値の場合とだいたい同じともいえるが・・・。”

![imageJRL8-14-8](/2024-01-13-QEUR23_VTRANS13/imageJRL8-14-8.jpg)

D先生 ： “いやあ、微妙だ・・・。でも、紫色の画素の量が少し増えているので、チェビシェフ処理の画像はユーグリッド処理の画像よりもカラフルになっています。数理的に考えて、**チェビシェフ距離の方が微細な差異の検出がやりやすい**のじゃないでしょうか。”

C部長 : “端子が抜けたのは、どの位置なんです？”

QEU:FOUNDER ： “上から2番目（真ん中）、左から3つ目の端子です。”

C部長 : “なるほど。そこは**「濃い青色の面積が小さくなっています」**ね。”

QEU:FOUNDER ： “そうです。両目法を使うことで、端子の高さを青色の面積に変換したわけです。もちろん、各端子の配色が違うので、多量のデータを使ってViTで学習させないと機械も理解しないとは思いますが・・・。”

D先生 ： “それでも覚えるポイントがより簡単になりました。画像認識の場合、人間がわかるものは機械でもわかるんですよね。“

### [＞寄付のお願い(click here)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

QEU:FOUNDER  ： “そういうこと。つぎは、今回の補足としてチェビシェフ距離を使ったSOART3合成画像を一通りみてみましょう。”


## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

D先生 ： “つまり、**製品の転写性**の問題ですね。 “

**（Attention Mapレイヤー：1）**

![imageJRL8-14-9](/2024-01-13-QEUR23_VTRANS13/imageJRL8-14-9.jpg)

QEU:FOUNDER ： “高い転写性を前提としている製品の外観検査において、豆の健康検査（dataset:beans）と同じ方法を使うことに対して、少しだけ「躊躇」があるんです。・・・確かにね、データを膨大につぎ込めばモデルが適当に、「なんとかしてくれる」とは思いながらもね・・・。”

C部長 : “その気持ちわかります。現場で手軽に使えるモデルをつくるために、学習データが過度に多くなっていくことへの懸念ですね。”

QEU:FOUNDER ： “さらに小生が気にしているのは、学習データよりも根本的な問題です。例えば、この製品（↓）を検査してください。D先生・・・。”

![imageJRL8-14-10](/2024-01-13-QEUR23_VTRANS13/imageJRL8-14-10.jpg)

D先生 ： “そうなんだよねえ・・・。**多くの工業部品には「色（の多様性）」がない**んですよ。「こんなんだったらグレースケールでの評価でいいんじゃない？」というくらいに・・・。”

QEU:FOUNDER ： “そうなるとグレースケールの画像は情報量が少ないんだよね。つまり、このままでは検出力が落ちていきます。ましてや、工業品の不良は非常に「微妙な」異常事象がおおい・・・。さらにいうと、その微妙な事象を**224x224の小さな（モノクロ）画像**で判断しなければならない。本来なら、1000x1000pixのカラー画像で判断したいところだよね。”

D先生 ： “最近、多くの会社さんから外観検査自動機が発売されていますが、結構「ムリゲー」をやっている可能性がありますね。もちろん、ベンダーもその辺は承知していて、やれないことを請け負うことはないとは思いますが・・・。・・・あっ、そうだ！だから、QEUシステムではSOART3法を使って、検査入力画像を無理やりカラーにしたんだ！”

QEU:FOUNDER ： “いままでの検討で分かってきていると思いますが、**SOART3を使えば物体の品質を「よりシンプルな少ない情報で」表現することが可能**なんです。”

D先生 ： “SOART3メトリックスって、ATTENTIONの計算（Q,K,V）のために作ったものだと思っていました。”

![imageJRL8-14-11](/2024-01-13-QEUR23_VTRANS13/imageJRL8-14-11.jpg)

QEU:FOUNDER ： “基本、予測に入力するメトリックスの情報は多ければ多いほどいいです。いままでのRT法が、感度(Y1:β)とSN比(Y2:η)の2次元だけというのがひどすぎるだけです。これが**3次元情報になるとRGB画像が描ける**とは思っていましたが、今回のViTで初めて使えるようになったわけです。”

D先生 ： “今回の我々の発明で、外観検査自動機は普及しますかねえ？ “

![imageJRL8-14-12](/2024-01-13-QEUR23_VTRANS13/imageJRL8-14-12.jpg)

QEU:FOUNDER ： “いまの**「品質保証の文化」**のままでは、（サプライアが検査機を導入することは）難しいと思うよ。”

C部長 : “いきなり、上に変な絵が出てきました。なんですか？コレ（↑）？”

